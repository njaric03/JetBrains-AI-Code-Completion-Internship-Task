[
    {
        "file_name": "abstract_model.py",
        "prefix": "import abc\nimport logging\nimport os\n\nimport numpy as np\n",
        "middle": "import pandas as pd",
        "suffix": "import torch\nimport torch.nn as nn\nfrom matplotlib import pyplot as plt\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport joblib\nfrom ta import add_all_ta_features\n\n\nclass AbstractModel(abc.ABC, nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.scaler_X = StandardScaler()\n        self.scaler_y = StandardScaler()\n        self.pca = None\n        self.model = None\n\n    @abc.abstractmethod\n    def build_model(self):\n        \"\"\"Build and return the actual model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def preprocess_data(self, data, fit=False):\n        \"\"\"Preprocess the data, including feature engineering, scaling, and PCA.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def train(self, train_data, val_data, num_epochs):\n        \"\"\"Train the model using the provided training and validation data.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def evaluate(self, test_data):\n        \"\"\"Evaluate the model's performance on test data.\"\"\"\n        pass\n\n    def save_model(self, path):\n        \"\"\"Save the trained model to a file.\"\"\"\n        if isinstance(self.model, nn.Module):\n            torch.save(self.model.state_dict(), path)\n        elif isinstance(self.model, BaseEstimator):\n            joblib.dump(self.model, path)\n        else:\n            raise NotImplementedError(\"Saving not implemented for this model type\")\n\n    def load_model(self, path):\n        \"\"\"Load a trained model from a file.\"\"\"\n        if isinstance(self.model, nn.Module):\n            self.model.load_state_dict(torch.load(path))\n        elif isinstance(self.model, BaseEstimator):\n            import joblib\n            self.model = joblib.load(path)\n        else:\n            raise NotImplementedError(\"Loading not implemented for this model type\")\n\n    def get_features_and_target(self, data):\n        target = self.config['target']\n        data['target'] = data[target].shift(-1)  # Shift the target to predict next minute's close\n\n        # Calculate technical indicators\n        data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n\n        # Drop the last row as it won't have a target value\n        data = data.dropna().reset_index(drop=True)\n\n        look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                                 'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n        # Drop OHLCV columns from the dataset, keeping only the indicators and target\n        feature_columns = [col for col in data.columns if col not in\n                           (['date', 'Open', 'High', 'Low', 'Volume', 'target'] + look_ahead_indicators)]\n\n        # Ensure all feature columns exist in the dataframe\n        feature_columns = [col for col in feature_columns if col in data.columns]\n\n        return data[feature_columns].values, data['target'].values\n\n\ndef set_up_folders():\n    project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\n    subfolder = os.path.join(project_root, 'results', 'outputs')\n    if not os.path.exists(subfolder):\n        os.makedirs(subfolder)\n    return project_root, subfolder\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n    # Check the type of scaler_y\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, y_batch in data_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred, _ = model(X_batch)\n\n            # Log shapes for debugging\n            logging.debug(f\"y_pred shape: {y_pred.shape}, y_batch shape: {y_batch.shape}\")\n\n            # Ensure y_pred and y_batch have the correct shape\n            y_pred = y_pred.view(-1, 1)\n            y_batch = y_batch.view(-1, 1)\n\n            # Convert to numpy and reshape if necessary\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy()\n\n            try:\n                # Convert predictions and targets back to the original scale\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                # Calculate the absolute error\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef save_experiment_results(training_time, avg_time_per_epoch, test_loss, avg_dollar_diff, data_limit, pca, csv_path):\n    results = {\n        'Training Time (seconds)': [f\"{training_time:.2f}\"],\n        'Average Time per Epoch (seconds)': [f\"{avg_time_per_epoch:.2f}\"],\n        'Test Loss': [f\"{test_loss:.6f}\"],\n        'Average Dollar Difference ($)': [f\"{avg_dollar_diff:.2f}\"],\n        'Data Limit': [data_limit],\n        'PCA': [pca]\n    }\n\n    df = pd.DataFrame(results)\n\n    # Check if the CSV file exists to append or create it\n    if os.path.exists(csv_path):\n        df.to_csv(csv_path, mode='a', header=False, index=False, float_format='%.6f')\n    else:\n        df.to_csv(csv_path, mode='w', header=True, index=False, float_format='%.6f')",
        "model_completion": "import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset",
        "Label": "partial"
    },
    {
        "file_name": "abstract_model.py",
        "prefix": "import abc\nimport logging\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom matplotlib import pyplot as plt\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport joblib\nfrom ta import add_all_ta_features\n\n\nclass AbstractModel(abc.ABC, nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.scaler_X = StandardScaler()\n        self.scaler_y = StandardScaler()\n        self.pca = None\n        self.model = None\n\n    @abc.abstractmethod\n    def build_model(self):\n        \"\"\"Build and return the actual model.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def preprocess_data(self, data, fit=False):\n        \"\"\"Preprocess the data, including feature engineering, scaling, and PCA.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def train(self, train_data, val_data, num_epochs):\n        \"\"\"Train the model using the provided training and validation data.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def evaluate(self, test_data):\n        \"\"\"Evaluate the model's performance on test data.\"\"\"\n        pass\n\n    def save_model(self, path):\n        \"\"\"Save the trained model to a file.\"\"\"\n        if isinstance(self.model, nn.Module):\n            torch.save(self.model.state_dict(), path)\n        elif isinstance(self.model, BaseEstimator):\n            joblib.dump(self.model, path)\n        else:\n            raise NotImplementedError(\"Saving not implemented for this model type\")\n\n    def load_model(self, path):\n        \"\"\"Load a trained model from a file.\"\"\"\n        if isinstance(self.model, nn.Module):\n            self.model.load_state_dict(torch.load(path))\n        elif isinstance(self.model, BaseEstimator):\n            import joblib\n            self.model = joblib.load(path)\n        else:\n            raise NotImplementedError(\"Loading not implemented for this model type\")\n\n    def get_features_and_target(self, data):\n        target = self.config['target']\n        data['target'] = data[target].shift(-1)  # Shift the target to predict next minute's close\n\n        # Calculate technical indicators\n        data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n\n        # Drop the last row as it won't have a target value\n        data = data.dropna().reset_index(drop=True)\n\n        look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                                 'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n        # Drop OHLCV columns from the dataset, keeping only the indicators and target\n        feature_columns = [col for col in data.columns if col not in\n                           (['date', 'Open', 'High', 'Low', 'Volume', 'target'] + look_ahead_indicators)]\n\n        # Ensure all feature columns exist in the dataframe\n        feature_columns = [col for col in feature_columns if col in data.columns]\n\n        return data[feature_columns].values, data['target'].values\n\n\ndef set_up_folders():\n    project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\n    subfolder = os.path.join(project_root, 'results', 'outputs')\n    if not os.path.exists(subfolder):\n        os.makedirs(subfolder)\n    return project_root, subfolder\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n    # Check the type of scaler_y\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, y_batch in data_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_pred, _ = model(X_batch)\n\n            # Log shapes for debugging\n            logging.debug(f\"y_pred shape: {y_pred.shape}, y_batch shape: {y_batch.shape}\")\n\n            # Ensure y_pred and y_batch have the correct shape\n            y_pred = y_pred.view(-1, 1)\n            y_batch = y_batch.view(-1, 1)\n\n            # Convert to numpy and reshape if necessary\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy()\n\n            try:\n                # Convert predictions and targets back to the original scale\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                # Calculate the absolute error\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n",
        "middle": "raise ValueError(\"No samples were processed\")",
        "suffix": "\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef save_experiment_results(training_time, avg_time_per_epoch, test_loss, avg_dollar_diff, data_limit, pca, csv_path):\n    results = {\n        'Training Time (seconds)': [f\"{training_time:.2f}\"],\n        'Average Time per Epoch (seconds)': [f\"{avg_time_per_epoch:.2f}\"],\n        'Test Loss': [f\"{test_loss:.6f}\"],\n        'Average Dollar Difference ($)': [f\"{avg_dollar_diff:.2f}\"],\n        'Data Limit': [data_limit],\n        'PCA': [pca]\n    }\n\n    df = pd.DataFrame(results)\n\n    # Check if the CSV file exists to append or create it\n    if os.path.exists(csv_path):\n        df.to_csv(csv_path, mode='a', header=False, index=False, float_format='%.6f')\n    else:\n        df.to_csv(csv_path, mode='w', header=True, index=False, float_format='%.6f')",
        "model_completion": "return 0\n\n    return total_abs_error / count\n\n\ndef plot",
        "Label": "incorrect"
    },
    {
        "file_name": "lin_regression.py",
        "prefix": "import logging\nimport os\nimport traceback\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom logic.models.lstm import subfolder, project_root\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results\n\nfrom abstract_model import AbstractModel, choose_n_components\n\n\nclass LinearRegressionModel(AbstractModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.scaler_X = MinMaxScaler()\n        self.scaler_y = MinMaxScaler()\n\n    def build_model(self):\n        self.model = LinearRegression()\n\n    def preprocess_data(self, data, fit=False):\n        X, y = self.get_features_and_target(data)\n\n        if fit:\n            X_scaled = self.scaler_X.fit_transform(X)\n            y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n\n            if self.config.get('use_pca', False):\n                n_components = choose_n_components(X_scaled,\n                                                   variance_threshold=self.config.get('variance_threshold', 0.95))\n                self.pca = PCA(n_components=n_components)\n                X_scaled = self.pca.fit_transform(X_scaled)\n        else:\n            X_scaled = self.scaler_X.transform(X)\n            y_scaled = self.scaler_y.transform(y.reshape(-1, 1)).flatten()\n\n            if self.pca is not None:\n                X_scaled = self.pca.transform(X_scaled)\n\n        return X_scaled, y_scaled\n\n    def train(self, train_data, val_data, num_epochs=1):\n        X_train, y_train = self.preprocess_data(train_data, fit=True)\n        X_val, y_val = self.preprocess_data(val_data, fit=False)\n\n        self.model.fit(X_train, y_train)\n\n        train_predictions = self.model.predict(X_train)\n        val_predictions = self.model.predict(X_val)\n\n        train_loss = mean_squared_error(y_train, train_predictions)\n        val_loss = mean_squared_error(y_val, val_predictions)\n\n        logging.info(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n\n        return train_loss, val_loss\n\n    def predict(self, val_data):\n        X_val, _ = self.preprocess_data(val_data, fit=False)\n        return self.model.predict(X_val)\n\n    def evaluate(self, test_data):\n        X_test, y_test = self.preprocess_data(test_data, fit=False)\n        y_pred = self.model.predict(X_test)\n        mse = mean_squared_error(y_test, y_pred)\n        return mse\n\n\ndef main(config_path):\n    config = load_config(config_path)\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n    # Define paths for datasets\n    datasets = {\n        'train': [os.path.join(project_root, 'data', path) for path in config['train_data']],\n        'val': os.path.join(project_root, 'data', config['val_data']),\n        'test': os.path.join(project_root, 'data', config['test_data'])\n    }\n\n    try:\n        logging.info(\"Starting main function\")\n",
        "middle": "logging.info(f\"Configuration loaded from: {config_path}\")",
        "suffix": "\n        # Load and process datasets\n        train_data = import_data(datasets['train'], limit=config['data_limit'])\n        val_data = import_data(os.path.join(project_root, 'data', config['val_data']), limit=config['data_limit'])\n        test_data = import_data(os.path.join(project_root, 'data', config['test_data']), limit=config['data_limit'])\n\n        # Initialize and train the model\n        model = LinearRegressionModel(config)\n        model.build_model()\n        train_loss, val_loss = model.train(train_data, val_data)\n\n        logging.info(f\"Training completed. Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n        # Evaluate on the test data\n        test_loss = model.evaluate(test_data)\n        logging.info(f\"Test Loss: {test_loss:.4f}\")\n\n        # Make predictions on validation data\n        val_predictions = model.predict(val_data)\n\n        # Inverse transform the predictions and actual values\n        val_actual = model.scaler_y.inverse_transform(\n            model.preprocess_data(val_data, fit=False)[1].reshape(-1, 1)).flatten()\n        val_predictions = model.scaler_y.inverse_transform(val_predictions.reshape(-1, 1)).flatten()\n\n        # Save and display results\n        save_and_display_results(val_actual, val_predictions, subfolder)\n        logging.info(f\"Results saved in {subfolder}\")\n\n        # Save the model\n        model.save_model(os.path.join(subfolder, 'linear_regression_model.joblib'))\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        traceback.print_exc()\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == '__main__':\n    config_path = '../../config/config.yaml'\n    main(config_path)",
        "model_completion": "model = LinearRegressionModel(config)\n        model.train(datasets['train",
        "Label": "incorrect"
    },
    {
        "file_name": "lin_regression.py",
        "prefix": "import logging\nimport os\nimport traceback\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom logic.models.lstm import subfolder, project_root\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results\n\nfrom abstract_model import AbstractModel, choose_n_components\n\n\nclass LinearRegressionModel(AbstractModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.scaler_X = MinMaxScaler()\n        self.scaler_y = MinMaxScaler()\n\n    def build_model(self):\n        self.model = LinearRegression()\n\n    def preprocess_data(self, data, fit=False):\n        X, y = self.get_features_and_target(data)\n\n        if fit:\n            X_scaled = self.scaler_X.fit_transform(X)\n            y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n\n            if self.config.get('use_pca', False):\n                n_components = choose_n_components(X_scaled,\n                                                   variance_threshold=self.config.get('variance_threshold', 0.95))\n                self.pca = PCA(n_components=n_components)\n                X_scaled = self.pca.fit_transform(X_scaled)\n        else:\n            X_scaled = self.scaler_X.transform(X)\n            y_scaled = self.scaler_y.transform(y.reshape(-1, 1)).flatten()\n\n            if self.pca is not None:\n                X_scaled = self.pca.transform(X_scaled)\n\n        return X_scaled, y_scaled\n\n    def train(self, train_data, val_data, num_epochs=1):\n        X_train, y_train = self.preprocess_data(train_data, fit=True)\n        X_val, y_val = self.preprocess_data(val_data, fit=False)\n\n        self.model.fit(X_train, y_train)\n\n        train_predictions = self.model.predict(X_train)\n        val_predictions = self.model.predict(X_val)\n\n        train_loss = mean_squared_error(y_train, train_predictions)\n        val_loss = mean_squared_error(y_val, val_predictions)\n\n",
        "middle": "logging.info(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')",
        "suffix": "\n        return train_loss, val_loss\n\n    def predict(self, val_data):\n        X_val, _ = self.preprocess_data(val_data, fit=False)\n        return self.model.predict(X_val)\n\n    def evaluate(self, test_data):\n        X_test, y_test = self.preprocess_data(test_data, fit=False)\n        y_pred = self.model.predict(X_test)\n        mse = mean_squared_error(y_test, y_pred)\n        return mse\n\n\ndef main(config_path):\n    config = load_config(config_path)\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n    # Define paths for datasets\n    datasets = {\n        'train': [os.path.join(project_root, 'data', path) for path in config['train_data']],\n        'val': os.path.join(project_root, 'data', config['val_data']),\n        'test': os.path.join(project_root, 'data', config['test_data'])\n    }\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n\n        # Load and process datasets\n        train_data = import_data(datasets['train'], limit=config['data_limit'])\n        val_data = import_data(os.path.join(project_root, 'data', config['val_data']), limit=config['data_limit'])\n        test_data = import_data(os.path.join(project_root, 'data', config['test_data']), limit=config['data_limit'])\n\n        # Initialize and train the model\n        model = LinearRegressionModel(config)\n        model.build_model()\n        train_loss, val_loss = model.train(train_data, val_data)\n\n        logging.info(f\"Training completed. Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n        # Evaluate on the test data\n        test_loss = model.evaluate(test_data)\n        logging.info(f\"Test Loss: {test_loss:.4f}\")\n\n        # Make predictions on validation data\n        val_predictions = model.predict(val_data)\n\n        # Inverse transform the predictions and actual values\n        val_actual = model.scaler_y.inverse_transform(\n            model.preprocess_data(val_data, fit=False)[1].reshape(-1, 1)).flatten()\n        val_predictions = model.scaler_y.inverse_transform(val_predictions.reshape(-1, 1)).flatten()\n\n        # Save and display results\n        save_and_display_results(val_actual, val_predictions, subfolder)\n        logging.info(f\"Results saved in {subfolder}\")\n\n        # Save the model\n        model.save_model(os.path.join(subfolder, 'linear_regression_model.joblib'))\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        traceback.print_exc()\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == '__main__':\n    config_path = '../../config/config.yaml'\n    main(config_path)",
        "model_completion": "return train_loss, val_loss\n\n    def predict(self, data):",
        "Label": "partial"
    },
    {
        "file_name": "lin_regression.py",
        "prefix": "import logging\nimport os\nimport traceback\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom logic.models.lstm import subfolder, project_root\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results\n\nfrom abstract_model import AbstractModel, choose_n_components\n\n\nclass LinearRegressionModel(AbstractModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.scaler_X = MinMaxScaler()\n        self.scaler_y = MinMaxScaler()\n\n    def build_model(self):\n        self.model = LinearRegression()\n\n    def preprocess_data(self, data, fit=False):\n        X, y = self.get_features_and_target(data)\n\n        if fit:\n            X_scaled = self.scaler_X.fit_transform(X)\n            y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n\n            if self.config.get('use_pca', False):\n                n_components = choose_n_components(X_scaled,\n                                                   variance_threshold=self.config.get('variance_threshold', 0.95))\n                self.pca = PCA(n_components=n_components)\n                X_scaled = self.pca.fit_transform(X_scaled)\n        else:\n            X_scaled = self.scaler_X.transform(X)\n            y_scaled = self.scaler_y.transform(y.reshape(-1, 1)).flatten()\n\n            if self.pca is not None:\n                X_scaled = self.pca.transform(X_scaled)\n\n        return X_scaled, y_scaled\n\n    def train(self, train_data, val_data, num_epochs=1):\n        X_train, y_train = self.preprocess_data(train_data, fit=True)\n        X_val, y_val = self.preprocess_data(val_data, fit=False)\n\n        self.model.fit(X_train, y_train)\n\n        train_predictions = self.model.predict(X_train)\n        val_predictions = self.model.predict(X_val)\n\n        train_loss = mean_squared_error(y_train, train_predictions)\n        val_loss = mean_squared_error(y_val, val_predictions)\n\n        logging.info(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n\n        return train_loss, val_loss\n\n    def predict(self, val_data):\n        X_val, _ = self.preprocess_data(val_data, fit=False)\n        return self.model.predict(X_val)\n\n",
        "middle": "def evaluate(self, test_data):",
        "suffix": "        X_test, y_test = self.preprocess_data(test_data, fit=False)\n        y_pred = self.model.predict(X_test)\n        mse = mean_squared_error(y_test, y_pred)\n        return mse\n\n\ndef main(config_path):\n    config = load_config(config_path)\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n    # Define paths for datasets\n    datasets = {\n        'train': [os.path.join(project_root, 'data', path) for path in config['train_data']],\n        'val': os.path.join(project_root, 'data', config['val_data']),\n        'test': os.path.join(project_root, 'data', config['test_data'])\n    }\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n\n        # Load and process datasets\n        train_data = import_data(datasets['train'], limit=config['data_limit'])\n        val_data = import_data(os.path.join(project_root, 'data', config['val_data']), limit=config['data_limit'])\n        test_data = import_data(os.path.join(project_root, 'data', config['test_data']), limit=config['data_limit'])\n\n        # Initialize and train the model\n        model = LinearRegressionModel(config)\n        model.build_model()\n        train_loss, val_loss = model.train(train_data, val_data)\n\n        logging.info(f\"Training completed. Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n        # Evaluate on the test data\n        test_loss = model.evaluate(test_data)\n        logging.info(f\"Test Loss: {test_loss:.4f}\")\n\n        # Make predictions on validation data\n        val_predictions = model.predict(val_data)\n\n        # Inverse transform the predictions and actual values\n        val_actual = model.scaler_y.inverse_transform(\n            model.preprocess_data(val_data, fit=False)[1].reshape(-1, 1)).flatten()\n        val_predictions = model.scaler_y.inverse_transform(val_predictions.reshape(-1, 1)).flatten()\n\n        # Save and display results\n        save_and_display_results(val_actual, val_predictions, subfolder)\n        logging.info(f\"Results saved in {subfolder}\")\n\n        # Save the model\n        model.save_model(os.path.join(subfolder, 'linear_regression_model.joblib'))\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        traceback.print_exc()\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == '__main__':\n    config_path = '../../config/config.yaml'\n    main(config_path)",
        "model_completion": "def save_model(self, model_name):\n        save_and_display_",
        "Label": "incorrect"
    },
    {
        "file_name": "lstm_dyn_attention_classification.py",
        "prefix": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nfrom ta import add_all_ta_features\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport logging\nimport traceback\nimport torch.optim.lr_scheduler as lr_scheduler\n\nfrom logic.models.abstract_model import set_up_folders, save_experiment_results\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results, save_and_display_results_classification\nfrom src.data_preprocessing.data_preprocessor import DataPreprocessor\n\nfrom ta.trend import MACD, EMAIndicator\nfrom ta.momentum import RSIIndicator\nfrom ta.volatility import BollingerBands\nfrom ta.volume import OnBalanceVolumeIndicator\n\nimport time\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\ntrain_processed, train_volatility, train_volume = None, None, None\nval_processed, val_volatility, val_volume = None, None, None\ntest_processed, test_volatility, test_volume = None, None, None\n\n\nclass DynamicAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(DynamicAttention, self).__init__()\n        self.feature_layer = nn.Linear(2, hidden_dim, bias=False)\n        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_out, volatility, volume):\n\n        features = torch.cat((volatility.unsqueeze(-1), volume.unsqueeze(-1)), dim=-1)\n        dynamic_weights = torch.tanh(self.feature_layer(features))\n        attention_weights = torch.softmax(self.attention(lstm_out * dynamic_weights).squeeze(-1), dim=1)\n        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n        return context_vector, attention_weights\n\n\nclass LSTMModel(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.0, use_attention=True):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.use_attention = use_attention\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n\n        self.attention = DynamicAttention(hidden_dim)\n\n        self.fc_layers = nn.Sequential(\n\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim),\n\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim // 2),\n\n            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim // 4),\n\n            nn.Linear(hidden_dim // 4, num_classes)\n        )\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x, volatility, volume):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n\n        lstm_out, _ = self.lstm(x, (h0, c0))\n\n\n        if self.use_attention:\n            context_vector, attention_weights = self.attention(lstm_out, volatility, volume)\n        else:\n            seq_len = lstm_out.size(1)\n            uniform_attention_weights = torch.ones(lstm_out.size(0), seq_len, device=lstm_out.device) / seq_len\n\n            context_vector = torch.sum(uniform_attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n            attention_weights = uniform_attention_weights\n\n        out = self.fc_layers(context_vector)\n        #out = self.softmax(out)\n        return out, attention_weights\n\n\nclass CryptoDataset(Dataset):\n    def __init__(self, data, volatility, volume, seq_length, absolute_prices):\n        self.data = torch.FloatTensor(data[:, :-1])\n        self.volatility = torch.FloatTensor(volatility)\n        self.volume = torch.FloatTensor(volume)\n        self.seq_length = seq_length\n        self.labels = torch.LongTensor(data[:, -1].astype(int))\n        self.absolute_prices = torch.FloatTensor(absolute_prices)\n\n        expected_length = len(self)\n        if len(self.labels) > expected_length:\n            self.labels = self.labels[-expected_length:]\n            self.absolute_prices = self.absolute_prices[-expected_length:]\n\n    def __len__(self):\n        return len(self.data) - self.seq_length + 1\n\n    def __getitem__(self, idx):\n        return (self.data[idx:idx + self.seq_length],\n                self.volatility[idx:idx + self.seq_length],\n                self.volume[idx:idx + self.seq_length],\n                self.labels[idx],\n                self.absolute_prices[idx])\n\ndef add_custom_ta_features(data):\n    # MACD\n    macd = MACD(close=data['Close'])\n    data['macd'] = macd.macd()\n    data['macd_signal'] = macd.macd_signal()\n    data['macd_diff'] = macd.macd_diff()\n\n    # EMA\n    data['ema_9'] = EMAIndicator(close=data['Close'], window=9).ema_indicator()\n    data['ema_21'] = EMAIndicator(close=data['Close'], window=21).ema_indicator()\n    data['ema_50'] = EMAIndicator(close=data['Close'], window=50).ema_indicator()\n    data['ema_200'] = EMAIndicator(close=data['Close'], window=200).ema_indicator()\n\n    # RSI\n    data['rsi_14'] = RSIIndicator(close=data['Close'], window=14).rsi()\n    data['rsi_21'] = RSIIndicator(close=data['Close'], window=21).rsi()\n\n    # Bollinger Bands\n    bb = BollingerBands(close=data['Close'])\n    data['bb_high'] = bb.bollinger_hband()\n    data['bb_low'] = bb.bollinger_lband()\n    data['bb_mid'] = bb.bollinger_mavg()\n    data['bb_width'] = (data['bb_high'] - data['bb_low']) / data['bb_mid']\n\n    # On-Balance Volume\n    data['obv'] = OnBalanceVolumeIndicator(close=data['Close'], volume=data['Volume']).on_balance_volume()\n\n    # Price rate of change\n    data['price_roc'] = data['Close'].pct_change(periods=12)\n\n    return data\n\n\ndef calculate_volatility(data, window_size=20):\n    data['log_return'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n    data['volatility'] = data['log_return'].rolling(window=window_size).std()\n    return data['volatility'].dropna()\n\ndef compute_grad_norms(model):\n    total_norm = 0.0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    total_norm = total_norm ** 0.5\n    return total_norm\n\ndef aggregate_and_save_cv_results(cv_results, subfolder):\n    all_test_actuals = []\n    all_test_predictions = []\n    avg_test_loss = 0\n\n    for result in cv_results:\n        all_test_actuals.extend(result['test_actuals'])\n        all_test_predictions.extend(result['test_predictions'])\n        avg_test_loss += result['test_loss']\n\n    avg_test_loss /= len(cv_results)\n\n    save_and_display_results_classification(all_test_actuals, all_test_predictions, subfolder,\n                                            dataset='test_aggregated')\n\n    with open(os.path.join(subfolder, 'cv_results.txt'), 'w') as f:\n        f.write(f\"Average Test Loss: {avg_test_loss:.6f}\\n\")\n\n    logging.info(f\"Aggregated cross-validation results saved. Average Test Loss: {avg_test_loss:.6f}\")\n\n\ndef preprocess_data(data: pd.DataFrame, config, data_preprocessor: DataPreprocessor):\n    target = config['target']\n\n    data = data.copy()\n\n    data['Close_diff'] = data['Close'].pct_change()\n\n    data['target'] = data[target].shift(-1)\n\n    data = data.dropna().reset_index(drop=True)\n\n    # data = add_custom_ta_features(data)\n    # data = data.dropna().reset_index(drop=True)\n    #\n    # feature_columns = ['macd', 'macd_signal', 'macd_diff',\n    #                    'ema_9', 'ema_21', 'ema_50', 'ema_200',\n    #                    'rsi_14', 'rsi_21',\n    #                    'bb_high', 'bb_low', 'bb_mid', 'bb_width',\n    #                    'obv', 'price_roc']\n    absolute_prices = data['Close'].values\n\n    data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n    data = data.dropna().reset_index(drop=True)\n\n    look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                             'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n    feature_columns = [col for col in data.columns if col not in (\n                ['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target',\n",
        "middle": "'Average_Close_diff'] + look_ahead_indicators)]",
        "suffix": "\n    logging.info(f\"Number of features before PCA: {len(feature_columns)}\")\n\n    data['volatility'] = calculate_volatility(data, window_size=config.get('volatility_window_size', 20))\n\n    data = data.drop(columns=['Close'])\n\n    data = data.dropna().reset_index(drop=True)\n\n    volatility = data['volatility']\n    volume = data['Volume']\n\n    X = data[feature_columns].values\n    y = data['target'].values\n\n    X_scaled, y_scaled, volatility_scaled, volume_scaled = data_preprocessor.fit_transform_data(X, y, volatility, volume,\n                                                                                                subfolder)\n\n    logging.info(f\"Number of features after preprocessing: {X_scaled.shape[1]}\")\n\n    assert not np.isnan(X_scaled).any(), \"NaN values found in features\"\n    assert not np.isnan(y_scaled).any(), \"NaN values found in target\"\n    assert not np.isnan(volatility_scaled).any(), \"NaN values found in volatility\"\n\n    return np.hstack((X_scaled, y_scaled.reshape(-1, 1))), volatility_scaled, volume_scaled, absolute_prices\n\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n    model.to(device)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, volatility_batch, volume_batch, y_batch, price_batch in train_loader:\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(device), volume_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n            assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        grad_norm = compute_grad_norms(model)\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch, price_batch in val_loader:\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(device), volume_batch.to(device), y_batch.to(device)\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n                assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n                loss = criterion(y_pred, y_batch)\n                val_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n\n        logging.info(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, Grad Norm: {grad_norm:.6f}')\n\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), os.path.join(subfolder, 'best_lstm_model.pth'))\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            logging.info(\"Early stopping triggered\")\n            break\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    actuals = []\n    predictions = []\n    absolute_prices = []\n    with torch.no_grad():\n        for inputs, volatility, volume, targets, prices in data_loader:\n            inputs, volatility, volume, targets = inputs.to(device), volatility.to(device), volume.to(device), targets.to(device)\n            outputs, _ = model(inputs, volatility, volume)\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n            actuals.extend(targets.cpu().numpy())\n            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n            absolute_prices.extend(prices.cpu().numpy())\n    return total_loss / len(data_loader), actuals, predictions, absolute_prices\n\n\ndef save_results_with_prices(actuals, predictions, absolute_prices, subfolder, dataset):\n    results_df = pd.DataFrame({\n        'Actual': actuals,\n        'Predicted': predictions,\n        'AbsolutePrice': absolute_prices\n    })\n    results_df.to_csv(os.path.join(subfolder, f'{dataset}_results_with_prices.csv'), index=False)\n    logging.info(f\"Results with absolute prices saved to {dataset}_results_with_prices.csv\")\n\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, volatility_batch, volume_batch, y_batch in data_loader:\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n            y_pred = y_pred[-len(y_batch):, :]\n\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy().reshape(-1, 1)\n\n            try:\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef setup_logging(subfolder):\n    log_filename = os.path.join(subfolder, f'experiment_log_{time.strftime(\"%Y%m%d-%H%M\")}.log')\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(log_filename),\n            logging.StreamHandler()  # This will also print to console\n        ]\n    )\n    return log_filename\n\n\ndef main(config_path, grid_search_run=None):\n    config = load_config(config_path)\n    global subfolder, train_processed, train_volatility, test_volume, test_volatility, test_processed, val_volume, val_volatility, val_processed, train_volume\n    global device\n    global project_root\n\n    project_root, subfolder = set_up_folders()\n\n    if grid_search_run is not None:\n        subfolder = os.path.join(subfolder, f'grid_search_{grid_search_run}')\n        os.makedirs(subfolder, exist_ok=True)\n\n    log_filename = setup_logging(subfolder)\n    logging.info(f\"Logging to file: {log_filename}\")\n\n    csv_path = os.path.join(subfolder, 'times.csv')\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n        logging.info(f\"Using device: {device}\")\n\n        training_time = 0\n        avg_time_per_epoch = 0\n\n        all_data_filenames = config['train_data'] + config['val_data'] + config['test_data']\n        all_data_paths = [os.path.join(project_root, 'data', path) for path in all_data_filenames]\n        all_data = import_data(all_data_paths, limit=config.get('data_limit', None))\n        all_data = all_data.sort_values('date').reset_index(drop=True)\n\n        scaler_type = config.get('scaler', 'standard')\n        use_pca = config.get('use_pca', False)\n\n        test_split = int(0.8 * len(all_data))\n        train_val_data = all_data[:test_split]\n        test_data = all_data[test_split:]\n\n        val_split = int(0.8 * len(train_val_data))\n        train_data = train_val_data[:val_split]\n        val_data = train_val_data[val_split:]\n\n        logging.info(f\"Data split - Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n\n        data_preprocessor = DataPreprocessor(scaler_type=scaler_type, use_pca=use_pca)\n\n        if train_processed is None:\n            train_processed, train_volatility, train_volume, train_prices = preprocess_data(train_data, config,\n                                                                                            data_preprocessor)\n            val_processed, val_volatility, val_volume, val_prices = preprocess_data(val_data, config, data_preprocessor)\n            test_processed, test_volatility, test_volume, test_prices = preprocess_data(test_data, config,\n                                                                                        data_preprocessor)\n\n        train_dataset = CryptoDataset(train_processed, train_volatility, train_volume, config['seq_length'],\n                                      train_prices)\n        val_dataset = CryptoDataset(val_processed, val_volatility, val_volume, config['seq_length'], val_prices)\n        test_dataset = CryptoDataset(test_processed, test_volatility, test_volume, config['seq_length'], test_prices)\n\n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, drop_last=True)\n        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], drop_last=True)\n        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], drop_last=True)\n\n        input_dim = train_processed.shape[1] - 1\n        hidden_dim = config['hidden_dim']\n        num_layers = config['num_layers']\n        dropout = config.get('dropout', 0)\n        use_attention = config['use_attention']\n        model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=3,\n                          dropout=dropout, use_attention=use_attention)\n        logging.info(f\"Model initialized with hidden_dim: {hidden_dim}, num_layers: {num_layers}, dropout: {dropout}\")\n\n        criterion = CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'],\n                                     weight_decay=config.get('weight_decay', 0))\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n        logging.info(\"Starting model training\")\n        start_time = time.time()\n        train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, config['num_epochs'])\n        end_time = time.time()\n        training_time = end_time - start_time\n        avg_time_per_epoch = training_time / config['num_epochs']\n        logging.info(\n            f\"Model training completed. Total time: {training_time:.2f}s, Avg time per epoch: {avg_time_per_epoch:.2f}s\")\n\n        logging.info(\"Starting model evaluation on test set\")\n        test_loss, test_actuals, test_predictions, test_prices = evaluate_model(model, test_loader, criterion, device)\n        logging.info(f\"Test evaluation completed. Test loss: {test_loss:.4f}\")\n\n        save_results_with_prices(test_actuals, test_predictions, test_prices, subfolder, dataset=f'test_pca_{use_pca}')\n\n        save_and_display_results_classification(test_actuals, test_predictions, subfolder, dataset=f'test_pca_{use_pca}')\n        logging.info(\"Results saved and displayed\")\n\n        save_experiment_results(training_time, avg_time_per_epoch, test_loss, 0.0, config.get('data_limit', 'N/A'),\n                                config.get('use_pca', False), csv_path)\n        logging.info(\"Experiment results saved\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        logging.error(traceback.format_exc())\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == \"__main__\":\n    config_path = '../../config/config.yaml'\n    main(config_path)",
        "model_completion": "'Close_diff', 'macd', 'macd_signal', 'macd_",
        "Label": "incorrect"
    },
    {
        "file_name": "lstm_dyn_attention_classification.py",
        "prefix": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nfrom ta import add_all_ta_features\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport logging\nimport traceback\nimport torch.optim.lr_scheduler as lr_scheduler\n\nfrom logic.models.abstract_model import set_up_folders, save_experiment_results\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results, save_and_display_results_classification\nfrom src.data_preprocessing.data_preprocessor import DataPreprocessor\n\nfrom ta.trend import MACD, EMAIndicator\nfrom ta.momentum import RSIIndicator\nfrom ta.volatility import BollingerBands\nfrom ta.volume import OnBalanceVolumeIndicator\n\nimport time\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\ntrain_processed, train_volatility, train_volume = None, None, None\nval_processed, val_volatility, val_volume = None, None, None\ntest_processed, test_volatility, test_volume = None, None, None\n\n\nclass DynamicAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(DynamicAttention, self).__init__()\n        self.feature_layer = nn.Linear(2, hidden_dim, bias=False)\n        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_out, volatility, volume):\n\n        features = torch.cat((volatility.unsqueeze(-1), volume.unsqueeze(-1)), dim=-1)\n        dynamic_weights = torch.tanh(self.feature_layer(features))\n        attention_weights = torch.softmax(self.attention(lstm_out * dynamic_weights).squeeze(-1), dim=1)\n        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n        return context_vector, attention_weights\n\n\nclass LSTMModel(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.0, use_attention=True):\n        super(LSTMModel, self).__init__()\n",
        "middle": "self.hidden_dim = hidden_dim",
        "suffix": "        self.num_layers = num_layers\n        self.use_attention = use_attention\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n\n        self.attention = DynamicAttention(hidden_dim)\n\n        self.fc_layers = nn.Sequential(\n\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim),\n\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim // 2),\n\n            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim // 4),\n\n            nn.Linear(hidden_dim // 4, num_classes)\n        )\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x, volatility, volume):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n\n        lstm_out, _ = self.lstm(x, (h0, c0))\n\n\n        if self.use_attention:\n            context_vector, attention_weights = self.attention(lstm_out, volatility, volume)\n        else:\n            seq_len = lstm_out.size(1)\n            uniform_attention_weights = torch.ones(lstm_out.size(0), seq_len, device=lstm_out.device) / seq_len\n\n            context_vector = torch.sum(uniform_attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n            attention_weights = uniform_attention_weights\n\n        out = self.fc_layers(context_vector)\n        #out = self.softmax(out)\n        return out, attention_weights\n\n\nclass CryptoDataset(Dataset):\n    def __init__(self, data, volatility, volume, seq_length, absolute_prices):\n        self.data = torch.FloatTensor(data[:, :-1])\n        self.volatility = torch.FloatTensor(volatility)\n        self.volume = torch.FloatTensor(volume)\n        self.seq_length = seq_length\n        self.labels = torch.LongTensor(data[:, -1].astype(int))\n        self.absolute_prices = torch.FloatTensor(absolute_prices)\n\n        expected_length = len(self)\n        if len(self.labels) > expected_length:\n            self.labels = self.labels[-expected_length:]\n            self.absolute_prices = self.absolute_prices[-expected_length:]\n\n    def __len__(self):\n        return len(self.data) - self.seq_length + 1\n\n    def __getitem__(self, idx):\n        return (self.data[idx:idx + self.seq_length],\n                self.volatility[idx:idx + self.seq_length],\n                self.volume[idx:idx + self.seq_length],\n                self.labels[idx],\n                self.absolute_prices[idx])\n\ndef add_custom_ta_features(data):\n    # MACD\n    macd = MACD(close=data['Close'])\n    data['macd'] = macd.macd()\n    data['macd_signal'] = macd.macd_signal()\n    data['macd_diff'] = macd.macd_diff()\n\n    # EMA\n    data['ema_9'] = EMAIndicator(close=data['Close'], window=9).ema_indicator()\n    data['ema_21'] = EMAIndicator(close=data['Close'], window=21).ema_indicator()\n    data['ema_50'] = EMAIndicator(close=data['Close'], window=50).ema_indicator()\n    data['ema_200'] = EMAIndicator(close=data['Close'], window=200).ema_indicator()\n\n    # RSI\n    data['rsi_14'] = RSIIndicator(close=data['Close'], window=14).rsi()\n    data['rsi_21'] = RSIIndicator(close=data['Close'], window=21).rsi()\n\n    # Bollinger Bands\n    bb = BollingerBands(close=data['Close'])\n    data['bb_high'] = bb.bollinger_hband()\n    data['bb_low'] = bb.bollinger_lband()\n    data['bb_mid'] = bb.bollinger_mavg()\n    data['bb_width'] = (data['bb_high'] - data['bb_low']) / data['bb_mid']\n\n    # On-Balance Volume\n    data['obv'] = OnBalanceVolumeIndicator(close=data['Close'], volume=data['Volume']).on_balance_volume()\n\n    # Price rate of change\n    data['price_roc'] = data['Close'].pct_change(periods=12)\n\n    return data\n\n\ndef calculate_volatility(data, window_size=20):\n    data['log_return'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n    data['volatility'] = data['log_return'].rolling(window=window_size).std()\n    return data['volatility'].dropna()\n\ndef compute_grad_norms(model):\n    total_norm = 0.0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    total_norm = total_norm ** 0.5\n    return total_norm\n\ndef aggregate_and_save_cv_results(cv_results, subfolder):\n    all_test_actuals = []\n    all_test_predictions = []\n    avg_test_loss = 0\n\n    for result in cv_results:\n        all_test_actuals.extend(result['test_actuals'])\n        all_test_predictions.extend(result['test_predictions'])\n        avg_test_loss += result['test_loss']\n\n    avg_test_loss /= len(cv_results)\n\n    save_and_display_results_classification(all_test_actuals, all_test_predictions, subfolder,\n                                            dataset='test_aggregated')\n\n    with open(os.path.join(subfolder, 'cv_results.txt'), 'w') as f:\n        f.write(f\"Average Test Loss: {avg_test_loss:.6f}\\n\")\n\n    logging.info(f\"Aggregated cross-validation results saved. Average Test Loss: {avg_test_loss:.6f}\")\n\n\ndef preprocess_data(data: pd.DataFrame, config, data_preprocessor: DataPreprocessor):\n    target = config['target']\n\n    data = data.copy()\n\n    data['Close_diff'] = data['Close'].pct_change()\n\n    data['target'] = data[target].shift(-1)\n\n    data = data.dropna().reset_index(drop=True)\n\n    # data = add_custom_ta_features(data)\n    # data = data.dropna().reset_index(drop=True)\n    #\n    # feature_columns = ['macd', 'macd_signal', 'macd_diff',\n    #                    'ema_9', 'ema_21', 'ema_50', 'ema_200',\n    #                    'rsi_14', 'rsi_21',\n    #                    'bb_high', 'bb_low', 'bb_mid', 'bb_width',\n    #                    'obv', 'price_roc']\n    absolute_prices = data['Close'].values\n\n    data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n    data = data.dropna().reset_index(drop=True)\n\n    look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                             'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n    feature_columns = [col for col in data.columns if col not in (\n                ['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target',\n                 'Average_Close_diff'] + look_ahead_indicators)]\n\n    logging.info(f\"Number of features before PCA: {len(feature_columns)}\")\n\n    data['volatility'] = calculate_volatility(data, window_size=config.get('volatility_window_size', 20))\n\n    data = data.drop(columns=['Close'])\n\n    data = data.dropna().reset_index(drop=True)\n\n    volatility = data['volatility']\n    volume = data['Volume']\n\n    X = data[feature_columns].values\n    y = data['target'].values\n\n    X_scaled, y_scaled, volatility_scaled, volume_scaled = data_preprocessor.fit_transform_data(X, y, volatility, volume,\n                                                                                                subfolder)\n\n    logging.info(f\"Number of features after preprocessing: {X_scaled.shape[1]}\")\n\n    assert not np.isnan(X_scaled).any(), \"NaN values found in features\"\n    assert not np.isnan(y_scaled).any(), \"NaN values found in target\"\n    assert not np.isnan(volatility_scaled).any(), \"NaN values found in volatility\"\n\n    return np.hstack((X_scaled, y_scaled.reshape(-1, 1))), volatility_scaled, volume_scaled, absolute_prices\n\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n    model.to(device)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, volatility_batch, volume_batch, y_batch, price_batch in train_loader:\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(device), volume_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n            assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        grad_norm = compute_grad_norms(model)\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch, price_batch in val_loader:\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(device), volume_batch.to(device), y_batch.to(device)\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n                assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n                loss = criterion(y_pred, y_batch)\n                val_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n\n        logging.info(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, Grad Norm: {grad_norm:.6f}')\n\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), os.path.join(subfolder, 'best_lstm_model.pth'))\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            logging.info(\"Early stopping triggered\")\n            break\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    actuals = []\n    predictions = []\n    absolute_prices = []\n    with torch.no_grad():\n        for inputs, volatility, volume, targets, prices in data_loader:\n            inputs, volatility, volume, targets = inputs.to(device), volatility.to(device), volume.to(device), targets.to(device)\n            outputs, _ = model(inputs, volatility, volume)\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n            actuals.extend(targets.cpu().numpy())\n            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n            absolute_prices.extend(prices.cpu().numpy())\n    return total_loss / len(data_loader), actuals, predictions, absolute_prices\n\n\ndef save_results_with_prices(actuals, predictions, absolute_prices, subfolder, dataset):\n    results_df = pd.DataFrame({\n        'Actual': actuals,\n        'Predicted': predictions,\n        'AbsolutePrice': absolute_prices\n    })\n    results_df.to_csv(os.path.join(subfolder, f'{dataset}_results_with_prices.csv'), index=False)\n    logging.info(f\"Results with absolute prices saved to {dataset}_results_with_prices.csv\")\n\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, volatility_batch, volume_batch, y_batch in data_loader:\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n            y_pred = y_pred[-len(y_batch):, :]\n\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy().reshape(-1, 1)\n\n            try:\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef setup_logging(subfolder):\n    log_filename = os.path.join(subfolder, f'experiment_log_{time.strftime(\"%Y%m%d-%H%M\")}.log')\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(log_filename),\n            logging.StreamHandler()  # This will also print to console\n        ]\n    )\n    return log_filename\n\n\ndef main(config_path, grid_search_run=None):\n    config = load_config(config_path)\n    global subfolder, train_processed, train_volatility, test_volume, test_volatility, test_processed, val_volume, val_volatility, val_processed, train_volume\n    global device\n    global project_root\n\n    project_root, subfolder = set_up_folders()\n\n    if grid_search_run is not None:\n        subfolder = os.path.join(subfolder, f'grid_search_{grid_search_run}')\n        os.makedirs(subfolder, exist_ok=True)\n\n    log_filename = setup_logging(subfolder)\n    logging.info(f\"Logging to file: {log_filename}\")\n\n    csv_path = os.path.join(subfolder, 'times.csv')\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n        logging.info(f\"Using device: {device}\")\n\n        training_time = 0\n        avg_time_per_epoch = 0\n\n        all_data_filenames = config['train_data'] + config['val_data'] + config['test_data']\n        all_data_paths = [os.path.join(project_root, 'data', path) for path in all_data_filenames]\n        all_data = import_data(all_data_paths, limit=config.get('data_limit', None))\n        all_data = all_data.sort_values('date').reset_index(drop=True)\n\n        scaler_type = config.get('scaler', 'standard')\n        use_pca = config.get('use_pca', False)\n\n        test_split = int(0.8 * len(all_data))\n        train_val_data = all_data[:test_split]\n        test_data = all_data[test_split:]\n\n        val_split = int(0.8 * len(train_val_data))\n        train_data = train_val_data[:val_split]\n        val_data = train_val_data[val_split:]\n\n        logging.info(f\"Data split - Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n\n        data_preprocessor = DataPreprocessor(scaler_type=scaler_type, use_pca=use_pca)\n\n        if train_processed is None:\n            train_processed, train_volatility, train_volume, train_prices = preprocess_data(train_data, config,\n                                                                                            data_preprocessor)\n            val_processed, val_volatility, val_volume, val_prices = preprocess_data(val_data, config, data_preprocessor)\n            test_processed, test_volatility, test_volume, test_prices = preprocess_data(test_data, config,\n                                                                                        data_preprocessor)\n\n        train_dataset = CryptoDataset(train_processed, train_volatility, train_volume, config['seq_length'],\n                                      train_prices)\n        val_dataset = CryptoDataset(val_processed, val_volatility, val_volume, config['seq_length'], val_prices)\n        test_dataset = CryptoDataset(test_processed, test_volatility, test_volume, config['seq_length'], test_prices)\n\n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, drop_last=True)\n        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], drop_last=True)\n        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], drop_last=True)\n\n        input_dim = train_processed.shape[1] - 1\n        hidden_dim = config['hidden_dim']\n        num_layers = config['num_layers']\n        dropout = config.get('dropout', 0)\n        use_attention = config['use_attention']\n        model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=3,\n                          dropout=dropout, use_attention=use_attention)\n        logging.info(f\"Model initialized with hidden_dim: {hidden_dim}, num_layers: {num_layers}, dropout: {dropout}\")\n\n        criterion = CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'],\n                                     weight_decay=config.get('weight_decay', 0))\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n        logging.info(\"Starting model training\")\n        start_time = time.time()\n        train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, config['num_epochs'])\n        end_time = time.time()\n        training_time = end_time - start_time\n        avg_time_per_epoch = training_time / config['num_epochs']\n        logging.info(\n            f\"Model training completed. Total time: {training_time:.2f}s, Avg time per epoch: {avg_time_per_epoch:.2f}s\")\n\n        logging.info(\"Starting model evaluation on test set\")\n        test_loss, test_actuals, test_predictions, test_prices = evaluate_model(model, test_loader, criterion, device)\n        logging.info(f\"Test evaluation completed. Test loss: {test_loss:.4f}\")\n\n        save_results_with_prices(test_actuals, test_predictions, test_prices, subfolder, dataset=f'test_pca_{use_pca}')\n\n        save_and_display_results_classification(test_actuals, test_predictions, subfolder, dataset=f'test_pca_{use_pca}')\n        logging.info(\"Results saved and displayed\")\n\n        save_experiment_results(training_time, avg_time_per_epoch, test_loss, 0.0, config.get('data_limit', 'N/A'),\n                                config.get('use_pca', False), csv_path)\n        logging.info(\"Experiment results saved\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        logging.error(traceback.format_exc())\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == \"__main__\":\n    config_path = '../../config/config.yaml'\n    main(config_path)",
        "model_completion": "self.hidden_dim = hidden_dim\n        self.num_layers = num_",
        "Label": "correct"
    },
    {
        "file_name": "lstm_dyn_attention_classification.py",
        "prefix": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nfrom ta import add_all_ta_features\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport logging\nimport traceback\nimport torch.optim.lr_scheduler as lr_scheduler\n\nfrom logic.models.abstract_model import set_up_folders, save_experiment_results\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results, save_and_display_results_classification\nfrom src.data_preprocessing.data_preprocessor import DataPreprocessor\n\nfrom ta.trend import MACD, EMAIndicator\nfrom ta.momentum import RSIIndicator\nfrom ta.volatility import BollingerBands\nfrom ta.volume import OnBalanceVolumeIndicator\n\nimport time\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\ntrain_processed, train_volatility, train_volume = None, None, None\nval_processed, val_volatility, val_volume = None, None, None\ntest_processed, test_volatility, test_volume = None, None, None\n\n\nclass DynamicAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(DynamicAttention, self).__init__()\n        self.feature_layer = nn.Linear(2, hidden_dim, bias=False)\n        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_out, volatility, volume):\n\n        features = torch.cat((volatility.unsqueeze(-1), volume.unsqueeze(-1)), dim=-1)\n        dynamic_weights = torch.tanh(self.feature_layer(features))\n        attention_weights = torch.softmax(self.attention(lstm_out * dynamic_weights).squeeze(-1), dim=1)\n        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n        return context_vector, attention_weights\n\n\nclass LSTMModel(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.0, use_attention=True):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.use_attention = use_attention\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n\n        self.attention = DynamicAttention(hidden_dim)\n\n        self.fc_layers = nn.Sequential(\n\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim),\n\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim // 2),\n\n            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim // 4),\n\n            nn.Linear(hidden_dim // 4, num_classes)\n        )\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x, volatility, volume):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n\n        lstm_out, _ = self.lstm(x, (h0, c0))\n\n\n        if self.use_attention:\n            context_vector, attention_weights = self.attention(lstm_out, volatility, volume)\n        else:\n            seq_len = lstm_out.size(1)\n            uniform_attention_weights = torch.ones(lstm_out.size(0), seq_len, device=lstm_out.device) / seq_len\n\n            context_vector = torch.sum(uniform_attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n            attention_weights = uniform_attention_weights\n\n        out = self.fc_layers(context_vector)\n        #out = self.softmax(out)\n        return out, attention_weights\n\n\nclass CryptoDataset(Dataset):\n    def __init__(self, data, volatility, volume, seq_length, absolute_prices):\n        self.data = torch.FloatTensor(data[:, :-1])\n        self.volatility = torch.FloatTensor(volatility)\n        self.volume = torch.FloatTensor(volume)\n        self.seq_length = seq_length\n        self.labels = torch.LongTensor(data[:, -1].astype(int))\n        self.absolute_prices = torch.FloatTensor(absolute_prices)\n\n        expected_length = len(self)\n        if len(self.labels) > expected_length:\n            self.labels = self.labels[-expected_length:]\n            self.absolute_prices = self.absolute_prices[-expected_length:]\n\n    def __len__(self):\n        return len(self.data) - self.seq_length + 1\n\n    def __getitem__(self, idx):\n        return (self.data[idx:idx + self.seq_length],\n                self.volatility[idx:idx + self.seq_length],\n                self.volume[idx:idx + self.seq_length],\n                self.labels[idx],\n                self.absolute_prices[idx])\n\ndef add_custom_ta_features(data):\n    # MACD\n    macd = MACD(close=data['Close'])\n    data['macd'] = macd.macd()\n    data['macd_signal'] = macd.macd_signal()\n    data['macd_diff'] = macd.macd_diff()\n\n    # EMA\n    data['ema_9'] = EMAIndicator(close=data['Close'], window=9).ema_indicator()\n    data['ema_21'] = EMAIndicator(close=data['Close'], window=21).ema_indicator()\n    data['ema_50'] = EMAIndicator(close=data['Close'], window=50).ema_indicator()\n    data['ema_200'] = EMAIndicator(close=data['Close'], window=200).ema_indicator()\n\n    # RSI\n    data['rsi_14'] = RSIIndicator(close=data['Close'], window=14).rsi()\n    data['rsi_21'] = RSIIndicator(close=data['Close'], window=21).rsi()\n\n    # Bollinger Bands\n    bb = BollingerBands(close=data['Close'])\n    data['bb_high'] = bb.bollinger_hband()\n    data['bb_low'] = bb.bollinger_lband()\n    data['bb_mid'] = bb.bollinger_mavg()\n    data['bb_width'] = (data['bb_high'] - data['bb_low']) / data['bb_mid']\n\n    # On-Balance Volume\n    data['obv'] = OnBalanceVolumeIndicator(close=data['Close'], volume=data['Volume']).on_balance_volume()\n\n    # Price rate of change\n    data['price_roc'] = data['Close'].pct_change(periods=12)\n\n    return data\n\n\ndef calculate_volatility(data, window_size=20):\n    data['log_return'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n    data['volatility'] = data['log_return'].rolling(window=window_size).std()\n    return data['volatility'].dropna()\n\ndef compute_grad_norms(model):\n    total_norm = 0.0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    total_norm = total_norm ** 0.5\n    return total_norm\n\ndef aggregate_and_save_cv_results(cv_results, subfolder):\n    all_test_actuals = []\n    all_test_predictions = []\n    avg_test_loss = 0\n\n    for result in cv_results:\n        all_test_actuals.extend(result['test_actuals'])\n        all_test_predictions.extend(result['test_predictions'])\n        avg_test_loss += result['test_loss']\n\n    avg_test_loss /= len(cv_results)\n\n    save_and_display_results_classification(all_test_actuals, all_test_predictions, subfolder,\n                                            dataset='test_aggregated')\n\n    with open(os.path.join(subfolder, 'cv_results.txt'), 'w') as f:\n        f.write(f\"Average Test Loss: {avg_test_loss:.6f}\\n\")\n\n    logging.info(f\"Aggregated cross-validation results saved. Average Test Loss: {avg_test_loss:.6f}\")\n\n\ndef preprocess_data(data: pd.DataFrame, config, data_preprocessor: DataPreprocessor):\n    target = config['target']\n\n    data = data.copy()\n\n    data['Close_diff'] = data['Close'].pct_change()\n\n    data['target'] = data[target].shift(-1)\n\n    data = data.dropna().reset_index(drop=True)\n\n    # data = add_custom_ta_features(data)\n    # data = data.dropna().reset_index(drop=True)\n    #\n    # feature_columns = ['macd', 'macd_signal', 'macd_diff',\n    #                    'ema_9', 'ema_21', 'ema_50', 'ema_200',\n    #                    'rsi_14', 'rsi_21',\n    #                    'bb_high', 'bb_low', 'bb_mid', 'bb_width',\n    #                    'obv', 'price_roc']\n    absolute_prices = data['Close'].values\n\n    data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n    data = data.dropna().reset_index(drop=True)\n\n    look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                             'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n    feature_columns = [col for col in data.columns if col not in (\n                ['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target',\n                 'Average_Close_diff'] + look_ahead_indicators)]\n\n    logging.info(f\"Number of features before PCA: {len(feature_columns)}\")\n\n    data['volatility'] = calculate_volatility(data, window_size=config.get('volatility_window_size', 20))\n\n    data = data.drop(columns=['Close'])\n\n    data = data.dropna().reset_index(drop=True)\n\n    volatility = data['volatility']\n    volume = data['Volume']\n\n    X = data[feature_columns].values\n    y = data['target'].values\n\n    X_scaled, y_scaled, volatility_scaled, volume_scaled = data_preprocessor.fit_transform_data(X, y, volatility, volume,\n                                                                                                subfolder)\n\n    logging.info(f\"Number of features after preprocessing: {X_scaled.shape[1]}\")\n\n    assert not np.isnan(X_scaled).any(), \"NaN values found in features\"\n    assert not np.isnan(y_scaled).any(), \"NaN values found in target\"\n    assert not np.isnan(volatility_scaled).any(), \"NaN values found in volatility\"\n\n    return np.hstack((X_scaled, y_scaled.reshape(-1, 1))), volatility_scaled, volume_scaled, absolute_prices\n\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n    model.to(device)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, volatility_batch, volume_batch, y_batch, price_batch in train_loader:\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(device), volume_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n            assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        grad_norm = compute_grad_norms(model)\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch, price_batch in val_loader:\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(device), volume_batch.to(device), y_batch.to(device)\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n                assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n                loss = criterion(y_pred, y_batch)\n                val_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n\n        logging.info(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, Grad Norm: {grad_norm:.6f}')\n\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), os.path.join(subfolder, 'best_lstm_model.pth'))\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            logging.info(\"Early stopping triggered\")\n            break\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    actuals = []\n    predictions = []\n    absolute_prices = []\n    with torch.no_grad():\n        for inputs, volatility, volume, targets, prices in data_loader:\n            inputs, volatility, volume, targets = inputs.to(device), volatility.to(device), volume.to(device), targets.to(device)\n            outputs, _ = model(inputs, volatility, volume)\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n            actuals.extend(targets.cpu().numpy())\n            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n            absolute_prices.extend(prices.cpu().numpy())\n    return total_loss / len(data_loader), actuals, predictions, absolute_prices\n\n\ndef save_results_with_prices(actuals, predictions, absolute_prices, subfolder, dataset):\n    results_df = pd.DataFrame({\n        'Actual': actuals,\n",
        "middle": "'Predicted': predictions,",
        "suffix": "        'AbsolutePrice': absolute_prices\n    })\n    results_df.to_csv(os.path.join(subfolder, f'{dataset}_results_with_prices.csv'), index=False)\n    logging.info(f\"Results with absolute prices saved to {dataset}_results_with_prices.csv\")\n\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, volatility_batch, volume_batch, y_batch in data_loader:\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n            y_pred = y_pred[-len(y_batch):, :]\n\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy().reshape(-1, 1)\n\n            try:\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef setup_logging(subfolder):\n    log_filename = os.path.join(subfolder, f'experiment_log_{time.strftime(\"%Y%m%d-%H%M\")}.log')\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(log_filename),\n            logging.StreamHandler()  # This will also print to console\n        ]\n    )\n    return log_filename\n\n\ndef main(config_path, grid_search_run=None):\n    config = load_config(config_path)\n    global subfolder, train_processed, train_volatility, test_volume, test_volatility, test_processed, val_volume, val_volatility, val_processed, train_volume\n    global device\n    global project_root\n\n    project_root, subfolder = set_up_folders()\n\n    if grid_search_run is not None:\n        subfolder = os.path.join(subfolder, f'grid_search_{grid_search_run}')\n        os.makedirs(subfolder, exist_ok=True)\n\n    log_filename = setup_logging(subfolder)\n    logging.info(f\"Logging to file: {log_filename}\")\n\n    csv_path = os.path.join(subfolder, 'times.csv')\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n        logging.info(f\"Using device: {device}\")\n\n        training_time = 0\n        avg_time_per_epoch = 0\n\n        all_data_filenames = config['train_data'] + config['val_data'] + config['test_data']\n        all_data_paths = [os.path.join(project_root, 'data', path) for path in all_data_filenames]\n        all_data = import_data(all_data_paths, limit=config.get('data_limit', None))\n        all_data = all_data.sort_values('date').reset_index(drop=True)\n\n        scaler_type = config.get('scaler', 'standard')\n        use_pca = config.get('use_pca', False)\n\n        test_split = int(0.8 * len(all_data))\n        train_val_data = all_data[:test_split]\n        test_data = all_data[test_split:]\n\n        val_split = int(0.8 * len(train_val_data))\n        train_data = train_val_data[:val_split]\n        val_data = train_val_data[val_split:]\n\n        logging.info(f\"Data split - Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n\n        data_preprocessor = DataPreprocessor(scaler_type=scaler_type, use_pca=use_pca)\n\n        if train_processed is None:\n            train_processed, train_volatility, train_volume, train_prices = preprocess_data(train_data, config,\n                                                                                            data_preprocessor)\n            val_processed, val_volatility, val_volume, val_prices = preprocess_data(val_data, config, data_preprocessor)\n            test_processed, test_volatility, test_volume, test_prices = preprocess_data(test_data, config,\n                                                                                        data_preprocessor)\n\n        train_dataset = CryptoDataset(train_processed, train_volatility, train_volume, config['seq_length'],\n                                      train_prices)\n        val_dataset = CryptoDataset(val_processed, val_volatility, val_volume, config['seq_length'], val_prices)\n        test_dataset = CryptoDataset(test_processed, test_volatility, test_volume, config['seq_length'], test_prices)\n\n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, drop_last=True)\n        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], drop_last=True)\n        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], drop_last=True)\n\n        input_dim = train_processed.shape[1] - 1\n        hidden_dim = config['hidden_dim']\n        num_layers = config['num_layers']\n        dropout = config.get('dropout', 0)\n        use_attention = config['use_attention']\n        model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=3,\n                          dropout=dropout, use_attention=use_attention)\n        logging.info(f\"Model initialized with hidden_dim: {hidden_dim}, num_layers: {num_layers}, dropout: {dropout}\")\n\n        criterion = CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'],\n                                     weight_decay=config.get('weight_decay', 0))\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n        logging.info(\"Starting model training\")\n        start_time = time.time()\n        train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, config['num_epochs'])\n        end_time = time.time()\n        training_time = end_time - start_time\n        avg_time_per_epoch = training_time / config['num_epochs']\n        logging.info(\n            f\"Model training completed. Total time: {training_time:.2f}s, Avg time per epoch: {avg_time_per_epoch:.2f}s\")\n\n        logging.info(\"Starting model evaluation on test set\")\n        test_loss, test_actuals, test_predictions, test_prices = evaluate_model(model, test_loader, criterion, device)\n        logging.info(f\"Test evaluation completed. Test loss: {test_loss:.4f}\")\n\n        save_results_with_prices(test_actuals, test_predictions, test_prices, subfolder, dataset=f'test_pca_{use_pca}')\n\n        save_and_display_results_classification(test_actuals, test_predictions, subfolder, dataset=f'test_pca_{use_pca}')\n        logging.info(\"Results saved and displayed\")\n\n        save_experiment_results(training_time, avg_time_per_epoch, test_loss, 0.0, config.get('data_limit', 'N/A'),\n                                config.get('use_pca', False), csv_path)\n        logging.info(\"Experiment results saved\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        logging.error(traceback.format_exc())\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == \"__main__\":\n    config_path = '../../config/config.yaml'\n    main(config_path)",
        "model_completion": "'Predicted': predictions,\n        'Absolute_Price': absolute_prices",
        "Label": "correct"
    },
    {
        "file_name": "lstm_dyn_attention_classification.py",
        "prefix": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nfrom ta import add_all_ta_features\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport logging\nimport traceback\nimport torch.optim.lr_scheduler as lr_scheduler\n\nfrom logic.models.abstract_model import set_up_folders, save_experiment_results\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results, save_and_display_results_classification\nfrom src.data_preprocessing.data_preprocessor import DataPreprocessor\n\nfrom ta.trend import MACD, EMAIndicator\nfrom ta.momentum import RSIIndicator\nfrom ta.volatility import BollingerBands\nfrom ta.volume import OnBalanceVolumeIndicator\n\nimport time\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\ntrain_processed, train_volatility, train_volume = None, None, None\nval_processed, val_volatility, val_volume = None, None, None\ntest_processed, test_volatility, test_volume = None, None, None\n\n\nclass DynamicAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(DynamicAttention, self).__init__()\n        self.feature_layer = nn.Linear(2, hidden_dim, bias=False)\n        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_out, volatility, volume):\n\n        features = torch.cat((volatility.unsqueeze(-1), volume.unsqueeze(-1)), dim=-1)\n        dynamic_weights = torch.tanh(self.feature_layer(features))\n        attention_weights = torch.softmax(self.attention(lstm_out * dynamic_weights).squeeze(-1), dim=1)\n        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n        return context_vector, attention_weights\n\n\nclass LSTMModel(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=0.0, use_attention=True):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.use_attention = use_attention\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n\n        self.attention = DynamicAttention(hidden_dim)\n\n        self.fc_layers = nn.Sequential(\n\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim),\n\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim // 2),\n\n            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n            nn.LeakyReLU(0.01),\n            nn.Dropout(dropout),\n            nn.BatchNorm1d(hidden_dim // 4),\n\n            nn.Linear(hidden_dim // 4, num_classes)\n        )\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x, volatility, volume):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n\n        lstm_out, _ = self.lstm(x, (h0, c0))\n\n\n        if self.use_attention:\n            context_vector, attention_weights = self.attention(lstm_out, volatility, volume)\n        else:\n            seq_len = lstm_out.size(1)\n            uniform_attention_weights = torch.ones(lstm_out.size(0), seq_len, device=lstm_out.device) / seq_len\n\n            context_vector = torch.sum(uniform_attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n            attention_weights = uniform_attention_weights\n\n        out = self.fc_layers(context_vector)\n        #out = self.softmax(out)\n        return out, attention_weights\n\n\nclass CryptoDataset(Dataset):\n    def __init__(self, data, volatility, volume, seq_length, absolute_prices):\n        self.data = torch.FloatTensor(data[:, :-1])\n        self.volatility = torch.FloatTensor(volatility)\n        self.volume = torch.FloatTensor(volume)\n        self.seq_length = seq_length\n        self.labels = torch.LongTensor(data[:, -1].astype(int))\n        self.absolute_prices = torch.FloatTensor(absolute_prices)\n\n        expected_length = len(self)\n        if len(self.labels) > expected_length:\n            self.labels = self.labels[-expected_length:]\n            self.absolute_prices = self.absolute_prices[-expected_length:]\n\n    def __len__(self):\n        return len(self.data) - self.seq_length + 1\n\n    def __getitem__(self, idx):\n        return (self.data[idx:idx + self.seq_length],\n                self.volatility[idx:idx + self.seq_length],\n                self.volume[idx:idx + self.seq_length],\n                self.labels[idx],\n                self.absolute_prices[idx])\n\ndef add_custom_ta_features(data):\n    # MACD\n    macd = MACD(close=data['Close'])\n    data['macd'] = macd.macd()\n    data['macd_signal'] = macd.macd_signal()\n    data['macd_diff'] = macd.macd_diff()\n\n    # EMA\n    data['ema_9'] = EMAIndicator(close=data['Close'], window=9).ema_indicator()\n    data['ema_21'] = EMAIndicator(close=data['Close'], window=21).ema_indicator()\n    data['ema_50'] = EMAIndicator(close=data['Close'], window=50).ema_indicator()\n    data['ema_200'] = EMAIndicator(close=data['Close'], window=200).ema_indicator()\n\n    # RSI\n    data['rsi_14'] = RSIIndicator(close=data['Close'], window=14).rsi()\n    data['rsi_21'] = RSIIndicator(close=data['Close'], window=21).rsi()\n\n    # Bollinger Bands\n    bb = BollingerBands(close=data['Close'])\n    data['bb_high'] = bb.bollinger_hband()\n    data['bb_low'] = bb.bollinger_lband()\n    data['bb_mid'] = bb.bollinger_mavg()\n    data['bb_width'] = (data['bb_high'] - data['bb_low']) / data['bb_mid']\n\n    # On-Balance Volume\n    data['obv'] = OnBalanceVolumeIndicator(close=data['Close'], volume=data['Volume']).on_balance_volume()\n\n    # Price rate of change\n    data['price_roc'] = data['Close'].pct_change(periods=12)\n\n    return data\n\n\ndef calculate_volatility(data, window_size=20):\n    data['log_return'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n    data['volatility'] = data['log_return'].rolling(window=window_size).std()\n    return data['volatility'].dropna()\n\n",
        "middle": "def compute_grad_norms(model):",
        "suffix": "    total_norm = 0.0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.item() ** 2\n    total_norm = total_norm ** 0.5\n    return total_norm\n\ndef aggregate_and_save_cv_results(cv_results, subfolder):\n    all_test_actuals = []\n    all_test_predictions = []\n    avg_test_loss = 0\n\n    for result in cv_results:\n        all_test_actuals.extend(result['test_actuals'])\n        all_test_predictions.extend(result['test_predictions'])\n        avg_test_loss += result['test_loss']\n\n    avg_test_loss /= len(cv_results)\n\n    save_and_display_results_classification(all_test_actuals, all_test_predictions, subfolder,\n                                            dataset='test_aggregated')\n\n    with open(os.path.join(subfolder, 'cv_results.txt'), 'w') as f:\n        f.write(f\"Average Test Loss: {avg_test_loss:.6f}\\n\")\n\n    logging.info(f\"Aggregated cross-validation results saved. Average Test Loss: {avg_test_loss:.6f}\")\n\n\ndef preprocess_data(data: pd.DataFrame, config, data_preprocessor: DataPreprocessor):\n    target = config['target']\n\n    data = data.copy()\n\n    data['Close_diff'] = data['Close'].pct_change()\n\n    data['target'] = data[target].shift(-1)\n\n    data = data.dropna().reset_index(drop=True)\n\n    # data = add_custom_ta_features(data)\n    # data = data.dropna().reset_index(drop=True)\n    #\n    # feature_columns = ['macd', 'macd_signal', 'macd_diff',\n    #                    'ema_9', 'ema_21', 'ema_50', 'ema_200',\n    #                    'rsi_14', 'rsi_21',\n    #                    'bb_high', 'bb_low', 'bb_mid', 'bb_width',\n    #                    'obv', 'price_roc']\n    absolute_prices = data['Close'].values\n\n    data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n    data = data.dropna().reset_index(drop=True)\n\n    look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                             'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n    feature_columns = [col for col in data.columns if col not in (\n                ['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target',\n                 'Average_Close_diff'] + look_ahead_indicators)]\n\n    logging.info(f\"Number of features before PCA: {len(feature_columns)}\")\n\n    data['volatility'] = calculate_volatility(data, window_size=config.get('volatility_window_size', 20))\n\n    data = data.drop(columns=['Close'])\n\n    data = data.dropna().reset_index(drop=True)\n\n    volatility = data['volatility']\n    volume = data['Volume']\n\n    X = data[feature_columns].values\n    y = data['target'].values\n\n    X_scaled, y_scaled, volatility_scaled, volume_scaled = data_preprocessor.fit_transform_data(X, y, volatility, volume,\n                                                                                                subfolder)\n\n    logging.info(f\"Number of features after preprocessing: {X_scaled.shape[1]}\")\n\n    assert not np.isnan(X_scaled).any(), \"NaN values found in features\"\n    assert not np.isnan(y_scaled).any(), \"NaN values found in target\"\n    assert not np.isnan(volatility_scaled).any(), \"NaN values found in volatility\"\n\n    return np.hstack((X_scaled, y_scaled.reshape(-1, 1))), volatility_scaled, volume_scaled, absolute_prices\n\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n    model.to(device)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, volatility_batch, volume_batch, y_batch, price_batch in train_loader:\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(device), volume_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n            assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        grad_norm = compute_grad_norms(model)\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch, price_batch in val_loader:\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(device), volume_batch.to(device), y_batch.to(device)\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n                assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n                loss = criterion(y_pred, y_batch)\n                val_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n\n        logging.info(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, Grad Norm: {grad_norm:.6f}')\n\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), os.path.join(subfolder, 'best_lstm_model.pth'))\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            logging.info(\"Early stopping triggered\")\n            break\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    actuals = []\n    predictions = []\n    absolute_prices = []\n    with torch.no_grad():\n        for inputs, volatility, volume, targets, prices in data_loader:\n            inputs, volatility, volume, targets = inputs.to(device), volatility.to(device), volume.to(device), targets.to(device)\n            outputs, _ = model(inputs, volatility, volume)\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n            actuals.extend(targets.cpu().numpy())\n            predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n            absolute_prices.extend(prices.cpu().numpy())\n    return total_loss / len(data_loader), actuals, predictions, absolute_prices\n\n\ndef save_results_with_prices(actuals, predictions, absolute_prices, subfolder, dataset):\n    results_df = pd.DataFrame({\n        'Actual': actuals,\n        'Predicted': predictions,\n        'AbsolutePrice': absolute_prices\n    })\n    results_df.to_csv(os.path.join(subfolder, f'{dataset}_results_with_prices.csv'), index=False)\n    logging.info(f\"Results with absolute prices saved to {dataset}_results_with_prices.csv\")\n\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, volatility_batch, volume_batch, y_batch in data_loader:\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)\n\n            y_pred = y_pred[-len(y_batch):, :]\n\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy().reshape(-1, 1)\n\n            try:\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef setup_logging(subfolder):\n    log_filename = os.path.join(subfolder, f'experiment_log_{time.strftime(\"%Y%m%d-%H%M\")}.log')\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(log_filename),\n            logging.StreamHandler()  # This will also print to console\n        ]\n    )\n    return log_filename\n\n\ndef main(config_path, grid_search_run=None):\n    config = load_config(config_path)\n    global subfolder, train_processed, train_volatility, test_volume, test_volatility, test_processed, val_volume, val_volatility, val_processed, train_volume\n    global device\n    global project_root\n\n    project_root, subfolder = set_up_folders()\n\n    if grid_search_run is not None:\n        subfolder = os.path.join(subfolder, f'grid_search_{grid_search_run}')\n        os.makedirs(subfolder, exist_ok=True)\n\n    log_filename = setup_logging(subfolder)\n    logging.info(f\"Logging to file: {log_filename}\")\n\n    csv_path = os.path.join(subfolder, 'times.csv')\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n        logging.info(f\"Using device: {device}\")\n\n        training_time = 0\n        avg_time_per_epoch = 0\n\n        all_data_filenames = config['train_data'] + config['val_data'] + config['test_data']\n        all_data_paths = [os.path.join(project_root, 'data', path) for path in all_data_filenames]\n        all_data = import_data(all_data_paths, limit=config.get('data_limit', None))\n        all_data = all_data.sort_values('date').reset_index(drop=True)\n\n        scaler_type = config.get('scaler', 'standard')\n        use_pca = config.get('use_pca', False)\n\n        test_split = int(0.8 * len(all_data))\n        train_val_data = all_data[:test_split]\n        test_data = all_data[test_split:]\n\n        val_split = int(0.8 * len(train_val_data))\n        train_data = train_val_data[:val_split]\n        val_data = train_val_data[val_split:]\n\n        logging.info(f\"Data split - Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n\n        data_preprocessor = DataPreprocessor(scaler_type=scaler_type, use_pca=use_pca)\n\n        if train_processed is None:\n            train_processed, train_volatility, train_volume, train_prices = preprocess_data(train_data, config,\n                                                                                            data_preprocessor)\n            val_processed, val_volatility, val_volume, val_prices = preprocess_data(val_data, config, data_preprocessor)\n            test_processed, test_volatility, test_volume, test_prices = preprocess_data(test_data, config,\n                                                                                        data_preprocessor)\n\n        train_dataset = CryptoDataset(train_processed, train_volatility, train_volume, config['seq_length'],\n                                      train_prices)\n        val_dataset = CryptoDataset(val_processed, val_volatility, val_volume, config['seq_length'], val_prices)\n        test_dataset = CryptoDataset(test_processed, test_volatility, test_volume, config['seq_length'], test_prices)\n\n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, drop_last=True)\n        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], drop_last=True)\n        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], drop_last=True)\n\n        input_dim = train_processed.shape[1] - 1\n        hidden_dim = config['hidden_dim']\n        num_layers = config['num_layers']\n        dropout = config.get('dropout', 0)\n        use_attention = config['use_attention']\n        model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=3,\n                          dropout=dropout, use_attention=use_attention)\n        logging.info(f\"Model initialized with hidden_dim: {hidden_dim}, num_layers: {num_layers}, dropout: {dropout}\")\n\n        criterion = CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'],\n                                     weight_decay=config.get('weight_decay', 0))\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n        logging.info(\"Starting model training\")\n        start_time = time.time()\n        train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, config['num_epochs'])\n        end_time = time.time()\n        training_time = end_time - start_time\n        avg_time_per_epoch = training_time / config['num_epochs']\n        logging.info(\n            f\"Model training completed. Total time: {training_time:.2f}s, Avg time per epoch: {avg_time_per_epoch:.2f}s\")\n\n        logging.info(\"Starting model evaluation on test set\")\n        test_loss, test_actuals, test_predictions, test_prices = evaluate_model(model, test_loader, criterion, device)\n        logging.info(f\"Test evaluation completed. Test loss: {test_loss:.4f}\")\n\n        save_results_with_prices(test_actuals, test_predictions, test_prices, subfolder, dataset=f'test_pca_{use_pca}')\n\n        save_and_display_results_classification(test_actuals, test_predictions, subfolder, dataset=f'test_pca_{use_pca}')\n        logging.info(\"Results saved and displayed\")\n\n        save_experiment_results(training_time, avg_time_per_epoch, test_loss, 0.0, config.get('data_limit', 'N/A'),\n                                config.get('use_pca', False), csv_path)\n        logging.info(\"Experiment results saved\")\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        logging.error(traceback.format_exc())\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == \"__main__\":\n    config_path = '../../config/config.yaml'\n    main(config_path)",
        "model_completion": "def calculate_volume(data, window_size=20):\n    data['volume",
        "Label": "incorrect"
    },
    {
        "file_name": "lstm_dyn_attention_regression.py",
        "prefix": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom ta import add_all_ta_features\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport logging\nimport traceback\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.decomposition import PCA\n\nfrom logic.models.abstract_model import set_up_folders, choose_n_components, save_experiment_results\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results\n\nimport time\n\n\nclass MeanAbsolutePercentageError(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_pred, y_true):\n        epsilon = 1e-8  # Small value to avoid division by zero\n        return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\nproject_root, subfolder = set_up_folders()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\n\nclass DynamicAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(DynamicAttention, self).__init__()\n        self.feature_layer = nn.Linear(2, hidden_dim, bias=False)\n        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_out, volatility, volume):\n        # Combine volatility and volume\n        features = torch.cat((volatility.unsqueeze(-1), volume.unsqueeze(-1)), dim=-1)\n        dynamic_weights = torch.tanh(self.feature_layer(features))\n        attention_weights = torch.softmax(self.attention(lstm_out * dynamic_weights).squeeze(-1), dim=1)\n        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n        return context_vector, attention_weights\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.attention = DynamicAttention(hidden_dim)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, volatility, volume):  # Added volume parameter\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        lstm_out, _ = self.lstm(x, (h0, c0))\n        context_vector, attention_weights = self.attention(lstm_out, volatility, volume)  # Added volume\n        out = self.fc(context_vector)\n        return out.view(-1, 1), attention_weights\n\n\nclass CryptoDataset(Dataset):\n    def __init__(self, data, volatility, volume, seq_length):  # Added volume parameter\n        self.data = torch.FloatTensor(data)\n        self.volatility = torch.FloatTensor(volatility)\n        self.volume = torch.FloatTensor(volume)\n        self.seq_length = seq_length\n\n    def __len__(self):\n        return len(self.data) - self.seq_length + 1\n\n    def __getitem__(self, idx):\n        return (self.data[idx:idx + self.seq_length, :-1],\n                self.volatility[idx:idx + self.seq_length],\n                self.volume[idx:idx + self.seq_length],  # New line\n                self.data[idx + self.seq_length - 1, -1])\n\n\ndef calculate_volatility(data, window_size=20):\n    data['log_return'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n    data['volatility'] = data['log_return'].rolling(window=window_size).std()\n    return data['volatility'].dropna()\n\n\ndef preprocess_data(data: pd.DataFrame, config, scaler_X=None, scaler_y=None, scaler_volatility=None,\n                    scaler_volume=None,\n                    pca=None, fit=False):\n    target = config['target']\n\n    # Calculate the difference in closing price\n    data['Close_diff'] = data['Close'].diff()\n\n    # Shift the target to predict the next period's price change\n    data['target'] = data['Close_diff'].shift(-1)\n\n    # Remove the first row which will have NaN for Close_diff\n    data = data.dropna().reset_index(drop=True)\n\n    data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n    data = data.dropna().reset_index(drop=True)\n\n    look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                             'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n    feature_columns = [col for col in data.columns if col not in\n                       (['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target'] + look_ahead_indicators)]\n\n    logging.info(f\"Number of features before PCA: {len(feature_columns)}\")\n\n    # Calculate volatility using the new method\n    data['volatility'] = calculate_volatility(data, window_size=config.get('volatility_window_size', 20))\n\n    # Drop the close column\n    data = data.drop(columns=['Close'])\n\n    # Drop rows with NaN values in volatility\n    data = data.dropna().reset_index(drop=True)\n\n    volatility = data['volatility']\n    volume = data['Volume']\n\n    X = data[feature_columns].values\n    y = data['target'].values\n\n    if not fit:\n        scaler_X = StandardScaler()\n        X_scaled = scaler_X.fit_transform(X)\n        # Save the scaler into a file for later use\n        if not os.path.exists(subfolder):\n            os.makedirs(subfolder)\n        torch.save(scaler_X, os.path.join(subfolder, 'scaler_X.pth'))\n\n        scaler_y = StandardScaler()\n        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n        torch.save(scaler_y, os.path.join(subfolder, 'scaler_y.pth'))\n\n        scaler_volatility = StandardScaler()\n        volatility_scaled = scaler_volatility.fit_transform(volatility.values.reshape(-1, 1)).flatten()\n        scaler_volume = StandardScaler()\n        volume_scaled = scaler_volume.fit_transform(volume.values.reshape(-1, 1)).flatten()\n\n        if config.get('use_pca', False):\n            logging.info(\"PCA is enabled. Determining optimal number of components...\")\n            n_components = choose_n_components(X_scaled,\n                                               variance_threshold=config.get('variance_threshold', 0.95))\n            pca = PCA(n_components=n_components)\n            X_scaled = pca.fit_transform(X_scaled)\n            logging.info(f\"PCA applied. Number of components: {n_components}\")\n            logging.info(f\"Variance explained by PCA: {sum(pca.explained_variance_ratio_):.4f}\")\n        else:\n            logging.info(\"PCA is not enabled.\")\n    else:\n        X_scaled = scaler_X.transform(X)\n        y_scaled = scaler_y.transform(y.reshape(-1, 1)).flatten()\n        volatility_scaled = scaler_volatility.transform(volatility.values.reshape(-1, 1)).flatten()\n        volume_scaled = scaler_volume.transform(volume.values.reshape(-1, 1)).flatten()\n\n        if pca is not None:\n            X_scaled = pca.transform(X_scaled)\n            logging.info(f\"PCA transform applied. Number of components: {pca.n_components_}\")\n\n    logging.info(f\"Number of features after preprocessing: {X_scaled.shape[1]}\")\n\n    # Ensure no NaN values\n    assert not np.isnan(X_scaled).any(), \"NaN values found in features\"\n    assert not np.isnan(y_scaled).any(), \"NaN values found in target\"\n    assert not np.isnan(volatility_scaled).any(), \"NaN values found in volatility\"\n\n    return (np.hstack((X_scaled, y_scaled.reshape(-1, 1))), volatility_scaled, volume_scaled,\n            scaler_X, scaler_y, scaler_volatility, scaler_volume, pca)\n\n\ndef train_model(model: nn.Module, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n    model.to(device)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, volatility_batch, volume_batch, y_batch in train_loader:  # Updated this line\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n            optimizer.zero_grad()\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n            # Ensure no NaN values in model output\n            assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n\n            loss = criterion(y_pred.squeeze(), y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch in val_loader:  # Updated this line\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                    device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n                # Ensure no NaN values in model output\n                assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n\n                loss = criterion(y_pred.squeeze(), y_batch)\n                val_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n\n        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), os.path.join(subfolder, 'best_lstm_model.pth'))\n        else:\n            patience_counter += 1\n\n",
        "middle": "if patience_counter >= patience:",
        "suffix": "            logging.info(\"Early stopping triggered\")\n            break\n\n        # completed_epochs += 1\n\n    # end_time = time.time()\n    # duration = end_time - start_time\n    # average_time_per_epoch = duration / completed_epochs if completed_epochs > 0 else 0\n    # print(f\"Training completed in {duration:.2f} seconds\")\n    # print(f\"Average time per epoch: {average_time_per_epoch:.2f} seconds\")\n\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, volatility, volume, targets in data_loader:  # Updated this line\n            inputs, volatility, volume, targets = inputs.to(device), volatility.to(device), volume.to(\n                device), targets.to(device)  # Updated this line\n            outputs, _ = model(inputs, volatility, volume)\n            loss = criterion(outputs.squeeze(), targets)\n            total_loss += loss.item()\n    return total_loss / len(data_loader)\n\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, volatility_batch, volume_batch, y_batch in data_loader:  # Updated this line\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n            logging.debug(f\"y_pred shape: {y_pred.shape}, y_batch shape: {y_batch.shape}\")\n\n            y_pred = y_pred.view(-1, 1)\n            y_batch = y_batch.view(-1, 1)\n\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy()\n\n            try:\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef main(config_path):\n    # Load configuration\n    config = load_config(config_path)\n\n    # Set up logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n    csv_path = os.path.join(subfolder, 'times.csv')\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n\n        # Define paths for datasets\n        datasets = {\n            'train': [os.path.join(project_root, 'data', path) for path in config['train_data']],\n            'val': [os.path.join(project_root, 'data', path) for path in config['val_data']],\n            'test': [os.path.join(project_root, 'data', path) for path in config['test_data']]\n        }\n\n        processed_data = {}\n        data_loaders = {}\n        scaler_X = None\n        scaler_y = None\n        scaler_volatility = None\n        pca = None\n\n        # Process each dataset (train, val, test)\n        for dataset_name, data_path in datasets.items():\n            logging.info(f\"Processing {dataset_name} dataset from {data_path}\")\n            data = import_data(data_path, limit=config.get('data_limit', None))\n            logging.info(f\"Data imported for {dataset_name}, shape: {data.shape}\")\n\n            if dataset_name == 'train':\n                processed_data[\n                    dataset_name], preprocessed_volatility, preprocessed_volume, scaler_X, scaler_y, scaler_volatility, scaler_volume, pca = preprocess_data(\n                    data, config, fit=False)\n            else:\n                processed_data[\n                    dataset_name], preprocessed_volatility, preprocessed_volume, _, _, _, _, _ = preprocess_data(data,\n                                                                                                                 config,\n                                                                                                                 scaler_X,\n                                                                                                                 scaler_y,\n                                                                                                                 scaler_volatility,\n                                                                                                                 scaler_volume,\n                                                                                                                 pca,\n                                                                                                                 fit=True)\n\n            logging.info(f\"Data preprocessed for {dataset_name}, shape: {processed_data[dataset_name].shape}\")\n\n            dataset = CryptoDataset(processed_data[dataset_name], preprocessed_volatility, preprocessed_volume,\n                                    seq_length=config['seq_length'])\n            data_loaders[dataset_name] = DataLoader(dataset, batch_size=config['batch_size'],\n                                                    shuffle=(dataset_name == 'train'))\n            logging.info(f\"DataLoader created for {dataset_name}\")\n        input_dim = processed_data['train'].shape[1] - 1  # Exclude target column\n        hidden_dim = config['hidden_dim']\n        num_layers = config['num_layers']\n        dropout = config['dropout']\n        model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=1,\n                          dropout=dropout)\n        logging.info(f\"Model initialized with hidden_dim: {hidden_dim}, num_layers: {num_layers}, dropout: {dropout}\")\n\n        # Define the loss function and optimizer\n        criterion = MeanAbsolutePercentageError()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n        logging.info(f\"Loss function, optimizer, and scheduler initialized. Learning rate: {config['learning_rate']}\")\n\n        # Train the model\n        logging.info(\"Starting model training\")\n        start_time = time.time()\n        train_model(model, data_loaders['train'], data_loaders['val'], criterion, optimizer, scheduler,\n                    config['num_epochs'])\n        end_time = time.time()\n        training_time = end_time - start_time\n        avg_time_per_epoch = training_time / config['num_epochs']\n        logging.info(\"Model training completed\")\n\n        # Evaluate the model on the test set\n        logging.info(\"Starting model evaluation on test set\")\n        model.load_state_dict(torch.load(os.path.join(subfolder, 'best_lstm_model.pth')))\n        model.eval()\n        test_loss = 0\n        test_actuals = []\n        test_predictions = []\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch in data_loaders['test']:  # Updated this line\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                    device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n                loss = criterion(y_pred.squeeze(), y_batch)\n                test_loss += loss.item()\n                test_actuals.extend(y_batch.cpu().numpy())\n                test_predictions.extend(y_pred.squeeze().cpu().numpy())\n        test_loss /= len(data_loaders['test'])\n        print(f'Test Loss: {test_loss:.6f}')\n\n        # Save and display results\n        logging.info(\"Saving and displaying results\")\n        save_and_display_results(test_actuals, test_predictions, subfolder)\n        average_dollar_difference = evaluate_dollar_difference(model, data_loaders['test'], scaler_y, device)\n        print(f'Average Dollar Difference: ${average_dollar_difference:.2f}')\n\n        save_experiment_results(\n            training_time, avg_time_per_epoch, test_loss, average_dollar_difference,\n            config.get('data_limit', 'N/A'), config.get('use_pca', False), csv_path\n        )\n\n        save_and_display_results(test_actuals, test_predictions, subfolder)\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        traceback.print_exc()\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == \"__main__\":\n    config_path = '../../config/config.yaml'\n    main(config_path)\n",
        "model_completion": "if patience_counter == patience:\n            break\n\n    return best_val",
        "Label": "correct"
    },
    {
        "file_name": "lstm_dyn_attention_regression.py",
        "prefix": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom ta import add_all_ta_features\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport logging\nimport traceback\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.decomposition import PCA\n\nfrom logic.models.abstract_model import set_up_folders, choose_n_components, save_experiment_results\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results\n\nimport time\n\n\nclass MeanAbsolutePercentageError(nn.Module):\n",
        "middle": "def __init__(self):",
        "suffix": "        super().__init__()\n\n    def forward(self, y_pred, y_true):\n        epsilon = 1e-8  # Small value to avoid division by zero\n        return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\nproject_root, subfolder = set_up_folders()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\n\nclass DynamicAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(DynamicAttention, self).__init__()\n        self.feature_layer = nn.Linear(2, hidden_dim, bias=False)\n        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_out, volatility, volume):\n        # Combine volatility and volume\n        features = torch.cat((volatility.unsqueeze(-1), volume.unsqueeze(-1)), dim=-1)\n        dynamic_weights = torch.tanh(self.feature_layer(features))\n        attention_weights = torch.softmax(self.attention(lstm_out * dynamic_weights).squeeze(-1), dim=1)\n        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n        return context_vector, attention_weights\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.attention = DynamicAttention(hidden_dim)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, volatility, volume):  # Added volume parameter\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        lstm_out, _ = self.lstm(x, (h0, c0))\n        context_vector, attention_weights = self.attention(lstm_out, volatility, volume)  # Added volume\n        out = self.fc(context_vector)\n        return out.view(-1, 1), attention_weights\n\n\nclass CryptoDataset(Dataset):\n    def __init__(self, data, volatility, volume, seq_length):  # Added volume parameter\n        self.data = torch.FloatTensor(data)\n        self.volatility = torch.FloatTensor(volatility)\n        self.volume = torch.FloatTensor(volume)\n        self.seq_length = seq_length\n\n    def __len__(self):\n        return len(self.data) - self.seq_length + 1\n\n    def __getitem__(self, idx):\n        return (self.data[idx:idx + self.seq_length, :-1],\n                self.volatility[idx:idx + self.seq_length],\n                self.volume[idx:idx + self.seq_length],  # New line\n                self.data[idx + self.seq_length - 1, -1])\n\n\ndef calculate_volatility(data, window_size=20):\n    data['log_return'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n    data['volatility'] = data['log_return'].rolling(window=window_size).std()\n    return data['volatility'].dropna()\n\n\ndef preprocess_data(data: pd.DataFrame, config, scaler_X=None, scaler_y=None, scaler_volatility=None,\n                    scaler_volume=None,\n                    pca=None, fit=False):\n    target = config['target']\n\n    # Calculate the difference in closing price\n    data['Close_diff'] = data['Close'].diff()\n\n    # Shift the target to predict the next period's price change\n    data['target'] = data['Close_diff'].shift(-1)\n\n    # Remove the first row which will have NaN for Close_diff\n    data = data.dropna().reset_index(drop=True)\n\n    data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n    data = data.dropna().reset_index(drop=True)\n\n    look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                             'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n    feature_columns = [col for col in data.columns if col not in\n                       (['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target'] + look_ahead_indicators)]\n\n    logging.info(f\"Number of features before PCA: {len(feature_columns)}\")\n\n    # Calculate volatility using the new method\n    data['volatility'] = calculate_volatility(data, window_size=config.get('volatility_window_size', 20))\n\n    # Drop the close column\n    data = data.drop(columns=['Close'])\n\n    # Drop rows with NaN values in volatility\n    data = data.dropna().reset_index(drop=True)\n\n    volatility = data['volatility']\n    volume = data['Volume']\n\n    X = data[feature_columns].values\n    y = data['target'].values\n\n    if not fit:\n        scaler_X = StandardScaler()\n        X_scaled = scaler_X.fit_transform(X)\n        # Save the scaler into a file for later use\n        if not os.path.exists(subfolder):\n            os.makedirs(subfolder)\n        torch.save(scaler_X, os.path.join(subfolder, 'scaler_X.pth'))\n\n        scaler_y = StandardScaler()\n        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n        torch.save(scaler_y, os.path.join(subfolder, 'scaler_y.pth'))\n\n        scaler_volatility = StandardScaler()\n        volatility_scaled = scaler_volatility.fit_transform(volatility.values.reshape(-1, 1)).flatten()\n        scaler_volume = StandardScaler()\n        volume_scaled = scaler_volume.fit_transform(volume.values.reshape(-1, 1)).flatten()\n\n        if config.get('use_pca', False):\n            logging.info(\"PCA is enabled. Determining optimal number of components...\")\n            n_components = choose_n_components(X_scaled,\n                                               variance_threshold=config.get('variance_threshold', 0.95))\n            pca = PCA(n_components=n_components)\n            X_scaled = pca.fit_transform(X_scaled)\n            logging.info(f\"PCA applied. Number of components: {n_components}\")\n            logging.info(f\"Variance explained by PCA: {sum(pca.explained_variance_ratio_):.4f}\")\n        else:\n            logging.info(\"PCA is not enabled.\")\n    else:\n        X_scaled = scaler_X.transform(X)\n        y_scaled = scaler_y.transform(y.reshape(-1, 1)).flatten()\n        volatility_scaled = scaler_volatility.transform(volatility.values.reshape(-1, 1)).flatten()\n        volume_scaled = scaler_volume.transform(volume.values.reshape(-1, 1)).flatten()\n\n        if pca is not None:\n            X_scaled = pca.transform(X_scaled)\n            logging.info(f\"PCA transform applied. Number of components: {pca.n_components_}\")\n\n    logging.info(f\"Number of features after preprocessing: {X_scaled.shape[1]}\")\n\n    # Ensure no NaN values\n    assert not np.isnan(X_scaled).any(), \"NaN values found in features\"\n    assert not np.isnan(y_scaled).any(), \"NaN values found in target\"\n    assert not np.isnan(volatility_scaled).any(), \"NaN values found in volatility\"\n\n    return (np.hstack((X_scaled, y_scaled.reshape(-1, 1))), volatility_scaled, volume_scaled,\n            scaler_X, scaler_y, scaler_volatility, scaler_volume, pca)\n\n\ndef train_model(model: nn.Module, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n    model.to(device)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, volatility_batch, volume_batch, y_batch in train_loader:  # Updated this line\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n            optimizer.zero_grad()\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n            # Ensure no NaN values in model output\n            assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n\n            loss = criterion(y_pred.squeeze(), y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch in val_loader:  # Updated this line\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                    device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n                # Ensure no NaN values in model output\n                assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n\n                loss = criterion(y_pred.squeeze(), y_batch)\n                val_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n\n        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), os.path.join(subfolder, 'best_lstm_model.pth'))\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            logging.info(\"Early stopping triggered\")\n            break\n\n        # completed_epochs += 1\n\n    # end_time = time.time()\n    # duration = end_time - start_time\n    # average_time_per_epoch = duration / completed_epochs if completed_epochs > 0 else 0\n    # print(f\"Training completed in {duration:.2f} seconds\")\n    # print(f\"Average time per epoch: {average_time_per_epoch:.2f} seconds\")\n\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, volatility, volume, targets in data_loader:  # Updated this line\n            inputs, volatility, volume, targets = inputs.to(device), volatility.to(device), volume.to(\n                device), targets.to(device)  # Updated this line\n            outputs, _ = model(inputs, volatility, volume)\n            loss = criterion(outputs.squeeze(), targets)\n            total_loss += loss.item()\n    return total_loss / len(data_loader)\n\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, volatility_batch, volume_batch, y_batch in data_loader:  # Updated this line\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n            logging.debug(f\"y_pred shape: {y_pred.shape}, y_batch shape: {y_batch.shape}\")\n\n            y_pred = y_pred.view(-1, 1)\n            y_batch = y_batch.view(-1, 1)\n\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy()\n\n            try:\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef main(config_path):\n    # Load configuration\n    config = load_config(config_path)\n\n    # Set up logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n    csv_path = os.path.join(subfolder, 'times.csv')\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n\n        # Define paths for datasets\n        datasets = {\n            'train': [os.path.join(project_root, 'data', path) for path in config['train_data']],\n            'val': [os.path.join(project_root, 'data', path) for path in config['val_data']],\n            'test': [os.path.join(project_root, 'data', path) for path in config['test_data']]\n        }\n\n        processed_data = {}\n        data_loaders = {}\n        scaler_X = None\n        scaler_y = None\n        scaler_volatility = None\n        pca = None\n\n        # Process each dataset (train, val, test)\n        for dataset_name, data_path in datasets.items():\n            logging.info(f\"Processing {dataset_name} dataset from {data_path}\")\n            data = import_data(data_path, limit=config.get('data_limit', None))\n            logging.info(f\"Data imported for {dataset_name}, shape: {data.shape}\")\n\n            if dataset_name == 'train':\n                processed_data[\n                    dataset_name], preprocessed_volatility, preprocessed_volume, scaler_X, scaler_y, scaler_volatility, scaler_volume, pca = preprocess_data(\n                    data, config, fit=False)\n            else:\n                processed_data[\n                    dataset_name], preprocessed_volatility, preprocessed_volume, _, _, _, _, _ = preprocess_data(data,\n                                                                                                                 config,\n                                                                                                                 scaler_X,\n                                                                                                                 scaler_y,\n                                                                                                                 scaler_volatility,\n                                                                                                                 scaler_volume,\n                                                                                                                 pca,\n                                                                                                                 fit=True)\n\n            logging.info(f\"Data preprocessed for {dataset_name}, shape: {processed_data[dataset_name].shape}\")\n\n            dataset = CryptoDataset(processed_data[dataset_name], preprocessed_volatility, preprocessed_volume,\n                                    seq_length=config['seq_length'])\n            data_loaders[dataset_name] = DataLoader(dataset, batch_size=config['batch_size'],\n                                                    shuffle=(dataset_name == 'train'))\n            logging.info(f\"DataLoader created for {dataset_name}\")\n        input_dim = processed_data['train'].shape[1] - 1  # Exclude target column\n        hidden_dim = config['hidden_dim']\n        num_layers = config['num_layers']\n        dropout = config['dropout']\n        model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=1,\n                          dropout=dropout)\n        logging.info(f\"Model initialized with hidden_dim: {hidden_dim}, num_layers: {num_layers}, dropout: {dropout}\")\n\n        # Define the loss function and optimizer\n        criterion = MeanAbsolutePercentageError()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n        logging.info(f\"Loss function, optimizer, and scheduler initialized. Learning rate: {config['learning_rate']}\")\n\n        # Train the model\n        logging.info(\"Starting model training\")\n        start_time = time.time()\n        train_model(model, data_loaders['train'], data_loaders['val'], criterion, optimizer, scheduler,\n                    config['num_epochs'])\n        end_time = time.time()\n        training_time = end_time - start_time\n        avg_time_per_epoch = training_time / config['num_epochs']\n        logging.info(\"Model training completed\")\n\n        # Evaluate the model on the test set\n        logging.info(\"Starting model evaluation on test set\")\n        model.load_state_dict(torch.load(os.path.join(subfolder, 'best_lstm_model.pth')))\n        model.eval()\n        test_loss = 0\n        test_actuals = []\n        test_predictions = []\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch in data_loaders['test']:  # Updated this line\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                    device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n                loss = criterion(y_pred.squeeze(), y_batch)\n                test_loss += loss.item()\n                test_actuals.extend(y_batch.cpu().numpy())\n                test_predictions.extend(y_pred.squeeze().cpu().numpy())\n        test_loss /= len(data_loaders['test'])\n        print(f'Test Loss: {test_loss:.6f}')\n\n        # Save and display results\n        logging.info(\"Saving and displaying results\")\n        save_and_display_results(test_actuals, test_predictions, subfolder)\n        average_dollar_difference = evaluate_dollar_difference(model, data_loaders['test'], scaler_y, device)\n        print(f'Average Dollar Difference: ${average_dollar_difference:.2f}')\n\n        save_experiment_results(\n            training_time, avg_time_per_epoch, test_loss, average_dollar_difference,\n            config.get('data_limit', 'N/A'), config.get('use_pca', False), csv_path\n        )\n\n        save_and_display_results(test_actuals, test_predictions, subfolder)\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        traceback.print_exc()\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == \"__main__\":\n    config_path = '../../config/config.yaml'\n    main(config_path)\n",
        "model_completion": "def __init__(self):\n        super(MeanAbsolutePercentageError, self",
        "Label": "correct"
    },
    {
        "file_name": "lstm_dyn_attention_regression.py",
        "prefix": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom ta import add_all_ta_features\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport logging\nimport traceback\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.decomposition import PCA\n\nfrom logic.models.abstract_model import set_up_folders, choose_n_components, save_experiment_results\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results\n\nimport time\n\n\nclass MeanAbsolutePercentageError(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_pred, y_true):\n        epsilon = 1e-8  # Small value to avoid division by zero\n        return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\nproject_root, subfolder = set_up_folders()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\n\nclass DynamicAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(DynamicAttention, self).__init__()\n        self.feature_layer = nn.Linear(2, hidden_dim, bias=False)\n        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_out, volatility, volume):\n        # Combine volatility and volume\n        features = torch.cat((volatility.unsqueeze(-1), volume.unsqueeze(-1)), dim=-1)\n        dynamic_weights = torch.tanh(self.feature_layer(features))\n        attention_weights = torch.softmax(self.attention(lstm_out * dynamic_weights).squeeze(-1), dim=1)\n        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n        return context_vector, attention_weights\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.attention = DynamicAttention(hidden_dim)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, volatility, volume):  # Added volume parameter\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        lstm_out, _ = self.lstm(x, (h0, c0))\n        context_vector, attention_weights = self.attention(lstm_out, volatility, volume)  # Added volume\n        out = self.fc(context_vector)\n        return out.view(-1, 1), attention_weights\n\n\nclass CryptoDataset(Dataset):\n    def __init__(self, data, volatility, volume, seq_length):  # Added volume parameter\n        self.data = torch.FloatTensor(data)\n        self.volatility = torch.FloatTensor(volatility)\n        self.volume = torch.FloatTensor(volume)\n        self.seq_length = seq_length\n\n    def __len__(self):\n        return len(self.data) - self.seq_length + 1\n\n    def __getitem__(self, idx):\n        return (self.data[idx:idx + self.seq_length, :-1],\n                self.volatility[idx:idx + self.seq_length],\n                self.volume[idx:idx + self.seq_length],  # New line\n                self.data[idx + self.seq_length - 1, -1])\n\n\ndef calculate_volatility(data, window_size=20):\n    data['log_return'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n    data['volatility'] = data['log_return'].rolling(window=window_size).std()\n    return data['volatility'].dropna()\n\n\ndef preprocess_data(data: pd.DataFrame, config, scaler_X=None, scaler_y=None, scaler_volatility=None,\n                    scaler_volume=None,\n                    pca=None, fit=False):\n    target = config['target']\n\n    # Calculate the difference in closing price\n    data['Close_diff'] = data['Close'].diff()\n\n    # Shift the target to predict the next period's price change\n    data['target'] = data['Close_diff'].shift(-1)\n\n    # Remove the first row which will have NaN for Close_diff\n    data = data.dropna().reset_index(drop=True)\n\n    data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n    data = data.dropna().reset_index(drop=True)\n\n    look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                             'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n    feature_columns = [col for col in data.columns if col not in\n                       (['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target'] + look_ahead_indicators)]\n\n    logging.info(f\"Number of features before PCA: {len(feature_columns)}\")\n\n    # Calculate volatility using the new method\n    data['volatility'] = calculate_volatility(data, window_size=config.get('volatility_window_size', 20))\n\n    # Drop the close column\n    data = data.drop(columns=['Close'])\n\n    # Drop rows with NaN values in volatility\n    data = data.dropna().reset_index(drop=True)\n\n    volatility = data['volatility']\n    volume = data['Volume']\n\n    X = data[feature_columns].values\n    y = data['target'].values\n\n    if not fit:\n        scaler_X = StandardScaler()\n        X_scaled = scaler_X.fit_transform(X)\n        # Save the scaler into a file for later use\n        if not os.path.exists(subfolder):\n            os.makedirs(subfolder)\n        torch.save(scaler_X, os.path.join(subfolder, 'scaler_X.pth'))\n\n        scaler_y = StandardScaler()\n        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n        torch.save(scaler_y, os.path.join(subfolder, 'scaler_y.pth'))\n\n        scaler_volatility = StandardScaler()\n        volatility_scaled = scaler_volatility.fit_transform(volatility.values.reshape(-1, 1)).flatten()\n        scaler_volume = StandardScaler()\n        volume_scaled = scaler_volume.fit_transform(volume.values.reshape(-1, 1)).flatten()\n\n        if config.get('use_pca', False):\n            logging.info(\"PCA is enabled. Determining optimal number of components...\")\n            n_components = choose_n_components(X_scaled,\n                                               variance_threshold=config.get('variance_threshold', 0.95))\n            pca = PCA(n_components=n_components)\n            X_scaled = pca.fit_transform(X_scaled)\n            logging.info(f\"PCA applied. Number of components: {n_components}\")\n            logging.info(f\"Variance explained by PCA: {sum(pca.explained_variance_ratio_):.4f}\")\n        else:\n            logging.info(\"PCA is not enabled.\")\n    else:\n        X_scaled = scaler_X.transform(X)\n        y_scaled = scaler_y.transform(y.reshape(-1, 1)).flatten()\n        volatility_scaled = scaler_volatility.transform(volatility.values.reshape(-1, 1)).flatten()\n        volume_scaled = scaler_volume.transform(volume.values.reshape(-1, 1)).flatten()\n\n        if pca is not None:\n            X_scaled = pca.transform(X_scaled)\n            logging.info(f\"PCA transform applied. Number of components: {pca.n_components_}\")\n\n    logging.info(f\"Number of features after preprocessing: {X_scaled.shape[1]}\")\n\n    # Ensure no NaN values\n    assert not np.isnan(X_scaled).any(), \"NaN values found in features\"\n    assert not np.isnan(y_scaled).any(), \"NaN values found in target\"\n    assert not np.isnan(volatility_scaled).any(), \"NaN values found in volatility\"\n\n    return (np.hstack((X_scaled, y_scaled.reshape(-1, 1))), volatility_scaled, volume_scaled,\n            scaler_X, scaler_y, scaler_volatility, scaler_volume, pca)\n\n\ndef train_model(model: nn.Module, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n    model.to(device)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, volatility_batch, volume_batch, y_batch in train_loader:  # Updated this line\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n            optimizer.zero_grad()\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n            # Ensure no NaN values in model output\n            assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n\n            loss = criterion(y_pred.squeeze(), y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch in val_loader:  # Updated this line\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                    device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n                # Ensure no NaN values in model output\n                assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n\n                loss = criterion(y_pred.squeeze(), y_batch)\n                val_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n\n        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), os.path.join(subfolder, 'best_lstm_model.pth'))\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            logging.info(\"Early stopping triggered\")\n            break\n\n        # completed_epochs += 1\n\n    # end_time = time.time()\n    # duration = end_time - start_time\n    # average_time_per_epoch = duration / completed_epochs if completed_epochs > 0 else 0\n    # print(f\"Training completed in {duration:.2f} seconds\")\n    # print(f\"Average time per epoch: {average_time_per_epoch:.2f} seconds\")\n\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, volatility, volume, targets in data_loader:  # Updated this line\n            inputs, volatility, volume, targets = inputs.to(device), volatility.to(device), volume.to(\n                device), targets.to(device)  # Updated this line\n            outputs, _ = model(inputs, volatility, volume)\n            loss = criterion(outputs.squeeze(), targets)\n            total_loss += loss.item()\n    return total_loss / len(data_loader)\n\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n",
        "middle": "total_abs_error = 0",
        "suffix": "    count = 0\n\n    if not isinstance(scaler_y, StandardScaler):\n        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, volatility_batch, volume_batch, y_batch in data_loader:  # Updated this line\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n            logging.debug(f\"y_pred shape: {y_pred.shape}, y_batch shape: {y_batch.shape}\")\n\n            y_pred = y_pred.view(-1, 1)\n            y_batch = y_batch.view(-1, 1)\n\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy()\n\n            try:\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef main(config_path):\n    # Load configuration\n    config = load_config(config_path)\n\n    # Set up logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n    csv_path = os.path.join(subfolder, 'times.csv')\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n\n        # Define paths for datasets\n        datasets = {\n            'train': [os.path.join(project_root, 'data', path) for path in config['train_data']],\n            'val': [os.path.join(project_root, 'data', path) for path in config['val_data']],\n            'test': [os.path.join(project_root, 'data', path) for path in config['test_data']]\n        }\n\n        processed_data = {}\n        data_loaders = {}\n        scaler_X = None\n        scaler_y = None\n        scaler_volatility = None\n        pca = None\n\n        # Process each dataset (train, val, test)\n        for dataset_name, data_path in datasets.items():\n            logging.info(f\"Processing {dataset_name} dataset from {data_path}\")\n            data = import_data(data_path, limit=config.get('data_limit', None))\n            logging.info(f\"Data imported for {dataset_name}, shape: {data.shape}\")\n\n            if dataset_name == 'train':\n                processed_data[\n                    dataset_name], preprocessed_volatility, preprocessed_volume, scaler_X, scaler_y, scaler_volatility, scaler_volume, pca = preprocess_data(\n                    data, config, fit=False)\n            else:\n                processed_data[\n                    dataset_name], preprocessed_volatility, preprocessed_volume, _, _, _, _, _ = preprocess_data(data,\n                                                                                                                 config,\n                                                                                                                 scaler_X,\n                                                                                                                 scaler_y,\n                                                                                                                 scaler_volatility,\n                                                                                                                 scaler_volume,\n                                                                                                                 pca,\n                                                                                                                 fit=True)\n\n            logging.info(f\"Data preprocessed for {dataset_name}, shape: {processed_data[dataset_name].shape}\")\n\n            dataset = CryptoDataset(processed_data[dataset_name], preprocessed_volatility, preprocessed_volume,\n                                    seq_length=config['seq_length'])\n            data_loaders[dataset_name] = DataLoader(dataset, batch_size=config['batch_size'],\n                                                    shuffle=(dataset_name == 'train'))\n            logging.info(f\"DataLoader created for {dataset_name}\")\n        input_dim = processed_data['train'].shape[1] - 1  # Exclude target column\n        hidden_dim = config['hidden_dim']\n        num_layers = config['num_layers']\n        dropout = config['dropout']\n        model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=1,\n                          dropout=dropout)\n        logging.info(f\"Model initialized with hidden_dim: {hidden_dim}, num_layers: {num_layers}, dropout: {dropout}\")\n\n        # Define the loss function and optimizer\n        criterion = MeanAbsolutePercentageError()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n        logging.info(f\"Loss function, optimizer, and scheduler initialized. Learning rate: {config['learning_rate']}\")\n\n        # Train the model\n        logging.info(\"Starting model training\")\n        start_time = time.time()\n        train_model(model, data_loaders['train'], data_loaders['val'], criterion, optimizer, scheduler,\n                    config['num_epochs'])\n        end_time = time.time()\n        training_time = end_time - start_time\n        avg_time_per_epoch = training_time / config['num_epochs']\n        logging.info(\"Model training completed\")\n\n        # Evaluate the model on the test set\n        logging.info(\"Starting model evaluation on test set\")\n        model.load_state_dict(torch.load(os.path.join(subfolder, 'best_lstm_model.pth')))\n        model.eval()\n        test_loss = 0\n        test_actuals = []\n        test_predictions = []\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch in data_loaders['test']:  # Updated this line\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                    device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n                loss = criterion(y_pred.squeeze(), y_batch)\n                test_loss += loss.item()\n                test_actuals.extend(y_batch.cpu().numpy())\n                test_predictions.extend(y_pred.squeeze().cpu().numpy())\n        test_loss /= len(data_loaders['test'])\n        print(f'Test Loss: {test_loss:.6f}')\n\n        # Save and display results\n        logging.info(\"Saving and displaying results\")\n        save_and_display_results(test_actuals, test_predictions, subfolder)\n        average_dollar_difference = evaluate_dollar_difference(model, data_loaders['test'], scaler_y, device)\n        print(f'Average Dollar Difference: ${average_dollar_difference:.2f}')\n\n        save_experiment_results(\n            training_time, avg_time_per_epoch, test_loss, average_dollar_difference,\n            config.get('data_limit', 'N/A'), config.get('use_pca', False), csv_path\n        )\n\n        save_and_display_results(test_actuals, test_predictions, subfolder)\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        traceback.print_exc()\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == \"__main__\":\n    config_path = '../../config/config.yaml'\n    main(config_path)\n",
        "model_completion": "total_loss = 0\n    total_dollar_difference = 0",
        "Label": "incorrect"
    },
    {
        "file_name": "lstm_dyn_attention_regression.py",
        "prefix": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom ta import add_all_ta_features\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport logging\nimport traceback\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom sklearn.decomposition import PCA\n\nfrom logic.models.abstract_model import set_up_folders, choose_n_components, save_experiment_results\nfrom src.data_preprocessing.data_importer import import_data\nfrom src.utils.config_loader import load_config\nfrom src.utils.data_saving_and_displaying import save_and_display_results\n\nimport time\n\n\nclass MeanAbsolutePercentageError(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_pred, y_true):\n        epsilon = 1e-8  # Small value to avoid division by zero\n        return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n\nproject_root, subfolder = set_up_folders()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n\n\nclass DynamicAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(DynamicAttention, self).__init__()\n        self.feature_layer = nn.Linear(2, hidden_dim, bias=False)\n        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_out, volatility, volume):\n        # Combine volatility and volume\n        features = torch.cat((volatility.unsqueeze(-1), volume.unsqueeze(-1)), dim=-1)\n        dynamic_weights = torch.tanh(self.feature_layer(features))\n        attention_weights = torch.softmax(self.attention(lstm_out * dynamic_weights).squeeze(-1), dim=1)\n        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_out, dim=1)\n        return context_vector, attention_weights\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.5):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        self.attention = DynamicAttention(hidden_dim)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, volatility, volume):  # Added volume parameter\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        lstm_out, _ = self.lstm(x, (h0, c0))\n        context_vector, attention_weights = self.attention(lstm_out, volatility, volume)  # Added volume\n        out = self.fc(context_vector)\n        return out.view(-1, 1), attention_weights\n\n\nclass CryptoDataset(Dataset):\n    def __init__(self, data, volatility, volume, seq_length):  # Added volume parameter\n        self.data = torch.FloatTensor(data)\n        self.volatility = torch.FloatTensor(volatility)\n        self.volume = torch.FloatTensor(volume)\n        self.seq_length = seq_length\n\n    def __len__(self):\n        return len(self.data) - self.seq_length + 1\n\n    def __getitem__(self, idx):\n        return (self.data[idx:idx + self.seq_length, :-1],\n                self.volatility[idx:idx + self.seq_length],\n                self.volume[idx:idx + self.seq_length],  # New line\n                self.data[idx + self.seq_length - 1, -1])\n\n\ndef calculate_volatility(data, window_size=20):\n    data['log_return'] = np.log(data['Close']) - np.log(data['Close'].shift(1))\n    data['volatility'] = data['log_return'].rolling(window=window_size).std()\n    return data['volatility'].dropna()\n\n\ndef preprocess_data(data: pd.DataFrame, config, scaler_X=None, scaler_y=None, scaler_volatility=None,\n                    scaler_volume=None,\n                    pca=None, fit=False):\n    target = config['target']\n\n    # Calculate the difference in closing price\n    data['Close_diff'] = data['Close'].diff()\n\n    # Shift the target to predict the next period's price change\n    data['target'] = data['Close_diff'].shift(-1)\n\n    # Remove the first row which will have NaN for Close_diff\n    data = data.dropna().reset_index(drop=True)\n\n    data = add_all_ta_features(data, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)\n    data = data.dropna().reset_index(drop=True)\n\n    look_ahead_indicators = ['trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a',\n                             'trend_visual_ichimoku_b', 'trend_stc', 'trend_psar_up', 'trend_psar_down']\n\n    feature_columns = [col for col in data.columns if col not in\n                       (['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target'] + look_ahead_indicators)]\n\n    logging.info(f\"Number of features before PCA: {len(feature_columns)}\")\n\n    # Calculate volatility using the new method\n    data['volatility'] = calculate_volatility(data, window_size=config.get('volatility_window_size', 20))\n\n    # Drop the close column\n    data = data.drop(columns=['Close'])\n\n    # Drop rows with NaN values in volatility\n    data = data.dropna().reset_index(drop=True)\n\n    volatility = data['volatility']\n    volume = data['Volume']\n\n    X = data[feature_columns].values\n    y = data['target'].values\n\n    if not fit:\n        scaler_X = StandardScaler()\n        X_scaled = scaler_X.fit_transform(X)\n        # Save the scaler into a file for later use\n        if not os.path.exists(subfolder):\n            os.makedirs(subfolder)\n        torch.save(scaler_X, os.path.join(subfolder, 'scaler_X.pth'))\n\n        scaler_y = StandardScaler()\n        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n        torch.save(scaler_y, os.path.join(subfolder, 'scaler_y.pth'))\n\n        scaler_volatility = StandardScaler()\n        volatility_scaled = scaler_volatility.fit_transform(volatility.values.reshape(-1, 1)).flatten()\n        scaler_volume = StandardScaler()\n        volume_scaled = scaler_volume.fit_transform(volume.values.reshape(-1, 1)).flatten()\n\n        if config.get('use_pca', False):\n            logging.info(\"PCA is enabled. Determining optimal number of components...\")\n            n_components = choose_n_components(X_scaled,\n                                               variance_threshold=config.get('variance_threshold', 0.95))\n            pca = PCA(n_components=n_components)\n            X_scaled = pca.fit_transform(X_scaled)\n            logging.info(f\"PCA applied. Number of components: {n_components}\")\n            logging.info(f\"Variance explained by PCA: {sum(pca.explained_variance_ratio_):.4f}\")\n        else:\n            logging.info(\"PCA is not enabled.\")\n    else:\n        X_scaled = scaler_X.transform(X)\n        y_scaled = scaler_y.transform(y.reshape(-1, 1)).flatten()\n        volatility_scaled = scaler_volatility.transform(volatility.values.reshape(-1, 1)).flatten()\n        volume_scaled = scaler_volume.transform(volume.values.reshape(-1, 1)).flatten()\n\n        if pca is not None:\n            X_scaled = pca.transform(X_scaled)\n            logging.info(f\"PCA transform applied. Number of components: {pca.n_components_}\")\n\n    logging.info(f\"Number of features after preprocessing: {X_scaled.shape[1]}\")\n\n    # Ensure no NaN values\n    assert not np.isnan(X_scaled).any(), \"NaN values found in features\"\n    assert not np.isnan(y_scaled).any(), \"NaN values found in target\"\n    assert not np.isnan(volatility_scaled).any(), \"NaN values found in volatility\"\n\n    return (np.hstack((X_scaled, y_scaled.reshape(-1, 1))), volatility_scaled, volume_scaled,\n            scaler_X, scaler_y, scaler_volatility, scaler_volume, pca)\n\n\ndef train_model(model: nn.Module, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n    model.to(device)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, volatility_batch, volume_batch, y_batch in train_loader:  # Updated this line\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n            optimizer.zero_grad()\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n            # Ensure no NaN values in model output\n            assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n\n            loss = criterion(y_pred.squeeze(), y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch in val_loader:  # Updated this line\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                    device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n                # Ensure no NaN values in model output\n                assert not torch.isnan(y_pred).any(), \"NaN values found in model output\"\n\n                loss = criterion(y_pred.squeeze(), y_batch)\n                val_loss += loss.item()\n\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n\n        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n\n        scheduler.step(val_loss)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), os.path.join(subfolder, 'best_lstm_model.pth'))\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            logging.info(\"Early stopping triggered\")\n            break\n\n        # completed_epochs += 1\n\n    # end_time = time.time()\n    # duration = end_time - start_time\n    # average_time_per_epoch = duration / completed_epochs if completed_epochs > 0 else 0\n    # print(f\"Training completed in {duration:.2f} seconds\")\n    # print(f\"Average time per epoch: {average_time_per_epoch:.2f} seconds\")\n\n\ndef evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, volatility, volume, targets in data_loader:  # Updated this line\n            inputs, volatility, volume, targets = inputs.to(device), volatility.to(device), volume.to(\n                device), targets.to(device)  # Updated this line\n            outputs, _ = model(inputs, volatility, volume)\n            loss = criterion(outputs.squeeze(), targets)\n            total_loss += loss.item()\n    return total_loss / len(data_loader)\n\n\ndef evaluate_dollar_difference(model, data_loader, scaler_y, device):\n    model.eval()\n    total_abs_error = 0\n    count = 0\n\n",
        "middle": "if not isinstance(scaler_y, StandardScaler):",
        "suffix": "        raise TypeError(f\"Expected StandardScaler, but got {type(scaler_y)}\")\n\n    with torch.no_grad():\n        for X_batch, volatility_batch, volume_batch, y_batch in data_loader:  # Updated this line\n            X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n            y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n\n            logging.debug(f\"y_pred shape: {y_pred.shape}, y_batch shape: {y_batch.shape}\")\n\n            y_pred = y_pred.view(-1, 1)\n            y_batch = y_batch.view(-1, 1)\n\n            y_pred_np = y_pred.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy()\n\n            try:\n                y_pred_unscaled = scaler_y.inverse_transform(y_pred_np)\n                y_batch_unscaled = scaler_y.inverse_transform(y_batch_np)\n\n                total_abs_error += np.sum(np.abs(y_pred_unscaled - y_batch_unscaled))\n                count += len(y_batch)\n            except ValueError as e:\n                logging.error(f\"Error in inverse transform: {str(e)}\")\n                logging.error(f\"y_pred_np shape: {y_pred_np.shape}, y_batch_np shape: {y_batch_np.shape}\")\n                raise\n\n    if count == 0:\n        raise ValueError(\"No samples were processed\")\n\n    average_dollar_diff = total_abs_error / count\n    return average_dollar_diff\n\n\ndef main(config_path):\n    # Load configuration\n    config = load_config(config_path)\n\n    # Set up logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n    csv_path = os.path.join(subfolder, 'times.csv')\n\n    try:\n        logging.info(\"Starting main function\")\n        logging.info(f\"Configuration loaded from: {config_path}\")\n\n        # Define paths for datasets\n        datasets = {\n            'train': [os.path.join(project_root, 'data', path) for path in config['train_data']],\n            'val': [os.path.join(project_root, 'data', path) for path in config['val_data']],\n            'test': [os.path.join(project_root, 'data', path) for path in config['test_data']]\n        }\n\n        processed_data = {}\n        data_loaders = {}\n        scaler_X = None\n        scaler_y = None\n        scaler_volatility = None\n        pca = None\n\n        # Process each dataset (train, val, test)\n        for dataset_name, data_path in datasets.items():\n            logging.info(f\"Processing {dataset_name} dataset from {data_path}\")\n            data = import_data(data_path, limit=config.get('data_limit', None))\n            logging.info(f\"Data imported for {dataset_name}, shape: {data.shape}\")\n\n            if dataset_name == 'train':\n                processed_data[\n                    dataset_name], preprocessed_volatility, preprocessed_volume, scaler_X, scaler_y, scaler_volatility, scaler_volume, pca = preprocess_data(\n                    data, config, fit=False)\n            else:\n                processed_data[\n                    dataset_name], preprocessed_volatility, preprocessed_volume, _, _, _, _, _ = preprocess_data(data,\n                                                                                                                 config,\n                                                                                                                 scaler_X,\n                                                                                                                 scaler_y,\n                                                                                                                 scaler_volatility,\n                                                                                                                 scaler_volume,\n                                                                                                                 pca,\n                                                                                                                 fit=True)\n\n            logging.info(f\"Data preprocessed for {dataset_name}, shape: {processed_data[dataset_name].shape}\")\n\n            dataset = CryptoDataset(processed_data[dataset_name], preprocessed_volatility, preprocessed_volume,\n                                    seq_length=config['seq_length'])\n            data_loaders[dataset_name] = DataLoader(dataset, batch_size=config['batch_size'],\n                                                    shuffle=(dataset_name == 'train'))\n            logging.info(f\"DataLoader created for {dataset_name}\")\n        input_dim = processed_data['train'].shape[1] - 1  # Exclude target column\n        hidden_dim = config['hidden_dim']\n        num_layers = config['num_layers']\n        dropout = config['dropout']\n        model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=1,\n                          dropout=dropout)\n        logging.info(f\"Model initialized with hidden_dim: {hidden_dim}, num_layers: {num_layers}, dropout: {dropout}\")\n\n        # Define the loss function and optimizer\n        criterion = MeanAbsolutePercentageError()\n        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n        logging.info(f\"Loss function, optimizer, and scheduler initialized. Learning rate: {config['learning_rate']}\")\n\n        # Train the model\n        logging.info(\"Starting model training\")\n        start_time = time.time()\n        train_model(model, data_loaders['train'], data_loaders['val'], criterion, optimizer, scheduler,\n                    config['num_epochs'])\n        end_time = time.time()\n        training_time = end_time - start_time\n        avg_time_per_epoch = training_time / config['num_epochs']\n        logging.info(\"Model training completed\")\n\n        # Evaluate the model on the test set\n        logging.info(\"Starting model evaluation on test set\")\n        model.load_state_dict(torch.load(os.path.join(subfolder, 'best_lstm_model.pth')))\n        model.eval()\n        test_loss = 0\n        test_actuals = []\n        test_predictions = []\n        with torch.no_grad():\n            for X_batch, volatility_batch, volume_batch, y_batch in data_loaders['test']:  # Updated this line\n                X_batch, volatility_batch, volume_batch, y_batch = X_batch.to(device), volatility_batch.to(\n                    device), volume_batch.to(device), y_batch.to(device)  # Updated this line\n                y_pred, _ = model(X_batch, volatility_batch, volume_batch)  # Updated this line\n                loss = criterion(y_pred.squeeze(), y_batch)\n                test_loss += loss.item()\n                test_actuals.extend(y_batch.cpu().numpy())\n                test_predictions.extend(y_pred.squeeze().cpu().numpy())\n        test_loss /= len(data_loaders['test'])\n        print(f'Test Loss: {test_loss:.6f}')\n\n        # Save and display results\n        logging.info(\"Saving and displaying results\")\n        save_and_display_results(test_actuals, test_predictions, subfolder)\n        average_dollar_difference = evaluate_dollar_difference(model, data_loaders['test'], scaler_y, device)\n        print(f'Average Dollar Difference: ${average_dollar_difference:.2f}')\n\n        save_experiment_results(\n            training_time, avg_time_per_epoch, test_loss, average_dollar_difference,\n            config.get('data_limit', 'N/A'), config.get('use_pca', False), csv_path\n        )\n\n        save_and_display_results(test_actuals, test_predictions, subfolder)\n\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        traceback.print_exc()\n\n    logging.info(\"Main completed\")\n\n\nif __name__ == \"__main__\":\n    config_path = '../../config/config.yaml'\n    main(config_path)\n",
        "model_completion": "with torch.no_grad():\n        for inputs, volatility, volume,",
        "Label": "incorrect"
    },
    {
        "file_name": "binance_scraper.py",
        "prefix": "import asyncio\nimport aiohttp\nimport os\nimport time\nimport ssl\n\nfrom src.data_preprocessing.unzipper import run\nfrom src.utils.config_loader import load_config\n\ndownload_dir = '../' + load_config('../config/config.yaml')['download_dir']\n\n\n",
        "middle": "async def download_url(session, url, file_path):",
        "suffix": "    try:\n        # Bypass SSL verification by setting ssl=False\n        async with session.get(url, ssl=False) as response:\n            if response.status == 200:\n                content = await response.read()\n                with open(file_path, 'wb') as file:\n                    file.write(content)\n                return len(content)\n    except ssl.SSLError as e:\n        print(f\"SSL Error for {url}: {e}\")\n    except aiohttp.ClientError as e:\n        print(f\"Client Error for {url}: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error for {url}: {e}\")\n    return 0\n\n\nasync def download_files(start_month=8, start_year=17, end_month=5, end_year=24, time_interval='3m'):\n    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n\n    start_time = time.time()\n    total_size = 0\n\n    async with aiohttp.ClientSession() as session:\n        tasks = []\n        for y in range(start_year, end_year + 1):\n            for m in range(1, 13):\n                if y == end_year and m > end_month:\n                    break\n                if y == start_year and m < start_month:\n                    continue\n\n                m = f'0{m}' if m < 10 else str(m)\n                file_name = f'BTCUSDT-{time_interval}-20{y}-{m}.zip'\n                file_path = os.path.join(download_dir, file_name)\n\n                if os.path.exists(file_path) or os.path.exists(file_path[:-4] + '.csv'):\n                    print(f'File {file_name} already exists')\n                    continue\n\n                url = f'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/{time_interval}/{file_name}'\n                tasks.append(download_url(session, url, file_path))\n\n        results = await asyncio.gather(*tasks)\n\n        success_count = sum(1 for size in results if size > 0)\n        total_size = sum(results)\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    print(f'Successfully downloaded {success_count} out of {len(tasks)} figures')\n    print(f'Total download time: {duration:.2f} seconds')\n    print(f'Total data downloaded: {total_size / (1024 * 1024):.2f} MB')\n    print(f'Average download speed: {total_size / (1024 * 1024) / duration:.2f} MB/s')\n\n\ndef main():\n    time_interval = input(\"Enter the time interval (1s, 3m, 5m...): \")\n    download_everything = input(\"Do you want to download all figures? (y/n): \").lower() == 'y'\n    if download_everything:\n        asyncio.run(download_files(time_interval=time_interval))\n    else:\n        start_year = int(input(\"Enter the last two digits of the start year: \"))\n        start_month = int(input(\"Enter the start month: \"))\n        end_year = int(input(\"Enter the last two digits of the end year: \"))\n        end_month = int(input(\"Enter the end month: \"))\n        asyncio.run(download_files(start_month, start_year, end_month, end_year, time_interval))\n    run()\n\n\nif __name__ == '__main__':\n    main()\n",
        "model_completion": "async def download_file(url, file_name):\n    async with aiohttp.",
        "Label": "incorrect"
    },
    {
        "file_name": "binance_scraper.py",
        "prefix": "import asyncio\nimport aiohttp\nimport os\nimport time\nimport ssl\n\nfrom src.data_preprocessing.unzipper import run\nfrom src.utils.config_loader import load_config\n\ndownload_dir = '../' + load_config('../config/config.yaml')['download_dir']\n\n\nasync def download_url(session, url, file_path):\n    try:\n        # Bypass SSL verification by setting ssl=False\n        async with session.get(url, ssl=False) as response:\n            if response.status == 200:\n                content = await response.read()\n                with open(file_path, 'wb') as file:\n                    file.write(content)\n                return len(content)\n    except ssl.SSLError as e:\n        print(f\"SSL Error for {url}: {e}\")\n    except aiohttp.ClientError as e:\n        print(f\"Client Error for {url}: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error for {url}: {e}\")\n    return 0\n\n\n",
        "middle": "async def download_files(start_month=8, start_year=17, end_month=5, end_year=24, time_interval='3m'):",
        "suffix": "    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n\n    start_time = time.time()\n    total_size = 0\n\n    async with aiohttp.ClientSession() as session:\n        tasks = []\n        for y in range(start_year, end_year + 1):\n            for m in range(1, 13):\n                if y == end_year and m > end_month:\n                    break\n                if y == start_year and m < start_month:\n                    continue\n\n                m = f'0{m}' if m < 10 else str(m)\n                file_name = f'BTCUSDT-{time_interval}-20{y}-{m}.zip'\n                file_path = os.path.join(download_dir, file_name)\n\n                if os.path.exists(file_path) or os.path.exists(file_path[:-4] + '.csv'):\n                    print(f'File {file_name} already exists')\n                    continue\n\n                url = f'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/{time_interval}/{file_name}'\n                tasks.append(download_url(session, url, file_path))\n\n        results = await asyncio.gather(*tasks)\n\n        success_count = sum(1 for size in results if size > 0)\n        total_size = sum(results)\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    print(f'Successfully downloaded {success_count} out of {len(tasks)} figures')\n    print(f'Total download time: {duration:.2f} seconds')\n    print(f'Total data downloaded: {total_size / (1024 * 1024):.2f} MB')\n    print(f'Average download speed: {total_size / (1024 * 1024) / duration:.2f} MB/s')\n\n\ndef main():\n    time_interval = input(\"Enter the time interval (1s, 3m, 5m...): \")\n    download_everything = input(\"Do you want to download all figures? (y/n): \").lower() == 'y'\n    if download_everything:\n        asyncio.run(download_files(time_interval=time_interval))\n    else:\n        start_year = int(input(\"Enter the last two digits of the start year: \"))\n        start_month = int(input(\"Enter the start month: \"))\n        end_year = int(input(\"Enter the last two digits of the end year: \"))\n        end_month = int(input(\"Enter the end month: \"))\n        asyncio.run(download_files(start_month, start_year, end_month, end_year, time_interval))\n    run()\n\n\nif __name__ == '__main__':\n    main()\n",
        "model_completion": "async def download_file(session, url, file_path):\n    try:",
        "Label": "incorrect"
    },
    {
        "file_name": "binance_scraper.py",
        "prefix": "import asyncio\nimport aiohttp\nimport os\nimport time\nimport ssl\n\nfrom src.data_preprocessing.unzipper import run\nfrom src.utils.config_loader import load_config\n\ndownload_dir = '../' + load_config('../config/config.yaml')['download_dir']\n\n\nasync def download_url(session, url, file_path):\n    try:\n        # Bypass SSL verification by setting ssl=False\n        async with session.get(url, ssl=False) as response:\n            if response.status == 200:\n                content = await response.read()\n                with open(file_path, 'wb') as file:\n                    file.write(content)\n                return len(content)\n    except ssl.SSLError as e:\n        print(f\"SSL Error for {url}: {e}\")\n    except aiohttp.ClientError as e:\n        print(f\"Client Error for {url}: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error for {url}: {e}\")\n    return 0\n\n\nasync def download_files(start_month=8, start_year=17, end_month=5, end_year=24, time_interval='3m'):\n    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n\n    start_time = time.time()\n    total_size = 0\n\n    async with aiohttp.ClientSession() as session:\n        tasks = []\n        for y in range(start_year, end_year + 1):\n            for m in range(1, 13):\n                if y == end_year and m > end_month:\n                    break\n                if y == start_year and m < start_month:\n                    continue\n\n                m = f'0{m}' if m < 10 else str(m)\n                file_name = f'BTCUSDT-{time_interval}-20{y}-{m}.zip'\n                file_path = os.path.join(download_dir, file_name)\n\n                if os.path.exists(file_path) or os.path.exists(file_path[:-4] + '.csv'):\n                    print(f'File {file_name} already exists')\n                    continue\n\n                url = f'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/{time_interval}/{file_name}'\n                tasks.append(download_url(session, url, file_path))\n\n        results = await asyncio.gather(*tasks)\n\n        success_count = sum(1 for size in results if size > 0)\n",
        "middle": "total_size = sum(results)",
        "suffix": "\n    end_time = time.time()\n    duration = end_time - start_time\n\n    print(f'Successfully downloaded {success_count} out of {len(tasks)} figures')\n    print(f'Total download time: {duration:.2f} seconds')\n    print(f'Total data downloaded: {total_size / (1024 * 1024):.2f} MB')\n    print(f'Average download speed: {total_size / (1024 * 1024) / duration:.2f} MB/s')\n\n\ndef main():\n    time_interval = input(\"Enter the time interval (1s, 3m, 5m...): \")\n    download_everything = input(\"Do you want to download all figures? (y/n): \").lower() == 'y'\n    if download_everything:\n        asyncio.run(download_files(time_interval=time_interval))\n    else:\n        start_year = int(input(\"Enter the last two digits of the start year: \"))\n        start_month = int(input(\"Enter the start month: \"))\n        end_year = int(input(\"Enter the last two digits of the end year: \"))\n        end_month = int(input(\"Enter the end month: \"))\n        asyncio.run(download_files(start_month, start_year, end_month, end_year, time_interval))\n    run()\n\n\nif __name__ == '__main__':\n    main()\n",
        "model_completion": "print(f'Downloaded {success_count} files, total size: {total_",
        "Label": "incorrect"
    },
    {
        "file_name": "binance_scraper.py",
        "prefix": "import asyncio\nimport aiohttp\nimport os\nimport time\nimport ssl\n\nfrom src.data_preprocessing.unzipper import run\nfrom src.utils.config_loader import load_config\n\ndownload_dir = '../' + load_config('../config/config.yaml')['download_dir']\n\n\nasync def download_url(session, url, file_path):\n    try:\n",
        "middle": "# Bypass SSL verification by setting ssl=False",
        "suffix": "        async with session.get(url, ssl=False) as response:\n            if response.status == 200:\n                content = await response.read()\n                with open(file_path, 'wb') as file:\n                    file.write(content)\n                return len(content)\n    except ssl.SSLError as e:\n        print(f\"SSL Error for {url}: {e}\")\n    except aiohttp.ClientError as e:\n        print(f\"Client Error for {url}: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error for {url}: {e}\")\n    return 0\n\n\nasync def download_files(start_month=8, start_year=17, end_month=5, end_year=24, time_interval='3m'):\n    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n\n    start_time = time.time()\n    total_size = 0\n\n    async with aiohttp.ClientSession() as session:\n        tasks = []\n        for y in range(start_year, end_year + 1):\n            for m in range(1, 13):\n                if y == end_year and m > end_month:\n                    break\n                if y == start_year and m < start_month:\n                    continue\n\n                m = f'0{m}' if m < 10 else str(m)\n                file_name = f'BTCUSDT-{time_interval}-20{y}-{m}.zip'\n                file_path = os.path.join(download_dir, file_name)\n\n                if os.path.exists(file_path) or os.path.exists(file_path[:-4] + '.csv'):\n                    print(f'File {file_name} already exists')\n                    continue\n\n                url = f'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/{time_interval}/{file_name}'\n                tasks.append(download_url(session, url, file_path))\n\n        results = await asyncio.gather(*tasks)\n\n        success_count = sum(1 for size in results if size > 0)\n        total_size = sum(results)\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    print(f'Successfully downloaded {success_count} out of {len(tasks)} figures')\n    print(f'Total download time: {duration:.2f} seconds')\n    print(f'Total data downloaded: {total_size / (1024 * 1024):.2f} MB')\n    print(f'Average download speed: {total_size / (1024 * 1024) / duration:.2f} MB/s')\n\n\ndef main():\n    time_interval = input(\"Enter the time interval (1s, 3m, 5m...): \")\n    download_everything = input(\"Do you want to download all figures? (y/n): \").lower() == 'y'\n    if download_everything:\n        asyncio.run(download_files(time_interval=time_interval))\n    else:\n        start_year = int(input(\"Enter the last two digits of the start year: \"))\n        start_month = int(input(\"Enter the start month: \"))\n        end_year = int(input(\"Enter the last two digits of the end year: \"))\n        end_month = int(input(\"Enter the end month: \"))\n        asyncio.run(download_files(start_month, start_year, end_month, end_year, time_interval))\n    run()\n\n\nif __name__ == '__main__':\n    main()\n",
        "model_completion": "async with session.get(url) as response:\n            if response.status ==",
        "Label": "incorrect"
    },
    {
        "file_name": "binance_scraper.py",
        "prefix": "import asyncio\nimport aiohttp\nimport os\nimport time\nimport ssl\n\nfrom src.data_preprocessing.unzipper import run\nfrom src.utils.config_loader import load_config\n\ndownload_dir = '../' + load_config('../config/config.yaml')['download_dir']\n\n\nasync def download_url(session, url, file_path):\n    try:\n        # Bypass SSL verification by setting ssl=False\n        async with session.get(url, ssl=False) as response:\n            if response.status == 200:\n                content = await response.read()\n                with open(file_path, 'wb') as file:\n                    file.write(content)\n                return len(content)\n    except ssl.SSLError as e:\n        print(f\"SSL Error for {url}: {e}\")\n    except aiohttp.ClientError as e:\n        print(f\"Client Error for {url}: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error for {url}: {e}\")\n    return 0\n\n\nasync def download_files(start_month=8, start_year=17, end_month=5, end_year=24, time_interval='3m'):\n    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n\n    start_time = time.time()\n    total_size = 0\n\n    async with aiohttp.ClientSession() as session:\n        tasks = []\n        for y in range(start_year, end_year + 1):\n            for m in range(1, 13):\n                if y == end_year and m > end_month:\n                    break\n                if y == start_year and m < start_month:\n                    continue\n\n",
        "middle": "m = f'0{m}' if m < 10 else str(m)",
        "suffix": "                file_name = f'BTCUSDT-{time_interval}-20{y}-{m}.zip'\n                file_path = os.path.join(download_dir, file_name)\n\n                if os.path.exists(file_path) or os.path.exists(file_path[:-4] + '.csv'):\n                    print(f'File {file_name} already exists')\n                    continue\n\n                url = f'https://data.binance.vision/data/spot/monthly/klines/BTCUSDT/{time_interval}/{file_name}'\n                tasks.append(download_url(session, url, file_path))\n\n        results = await asyncio.gather(*tasks)\n\n        success_count = sum(1 for size in results if size > 0)\n        total_size = sum(results)\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    print(f'Successfully downloaded {success_count} out of {len(tasks)} figures')\n    print(f'Total download time: {duration:.2f} seconds')\n    print(f'Total data downloaded: {total_size / (1024 * 1024):.2f} MB')\n    print(f'Average download speed: {total_size / (1024 * 1024) / duration:.2f} MB/s')\n\n\ndef main():\n    time_interval = input(\"Enter the time interval (1s, 3m, 5m...): \")\n    download_everything = input(\"Do you want to download all figures? (y/n): \").lower() == 'y'\n    if download_everything:\n        asyncio.run(download_files(time_interval=time_interval))\n    else:\n        start_year = int(input(\"Enter the last two digits of the start year: \"))\n        start_month = int(input(\"Enter the start month: \"))\n        end_year = int(input(\"Enter the last two digits of the end year: \"))\n        end_month = int(input(\"Enter the end month: \"))\n        asyncio.run(download_files(start_month, start_year, end_month, end_year, time_interval))\n    run()\n\n\nif __name__ == '__main__':\n    main()\n",
        "model_completion": "if time_interval == '3m':\n                    if m == 1 or m",
        "Label": "incorrect"
    },
    {
        "file_name": "calculate_profit.py",
        "prefix": "import pandas as pd\nimport numpy as np\nfrom typing import Tuple, List\n\n\ndef read_data(file_path: str) -> pd.DataFrame:\n    try:\n        data = pd.read_csv(file_path)\n        return pd.DataFrame(data)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return pd.DataFrame()\n    except pd.errors.EmptyDataError:\n        print(f\"Error: The file at {file_path} is empty\")\n        return pd.DataFrame()\n    except pd.errors.ParserError:\n        print(f\"Error: Unable to parse the file at {file_path}\")\n        return pd.DataFrame()\n\n\ndef calculate_metrics(returns: List[float]) -> Tuple[float, float, float]:\n    returns = pd.Series(returns)\n    sharpe_ratio = np.sqrt(252) * returns.mean() / returns.std()\n    max_drawdown = (returns.cumsum() - returns.cumsum().cummax()).min()\n    total_return = (1 + returns).prod() - 1\n    return sharpe_ratio, max_drawdown, total_return\n\n\ndef evaluate_strategy(df: pd.DataFrame, initial_investment: float = 1000) -> Tuple[float, float, pd.DataFrame]:\n    holding = False\n    entry_price = 0\n    capital = initial_investment\n    trades = []\n    returns = []\n\n    for index, row in df.iterrows():\n        if row[\"Predicted\"] == 2 and not holding:\n            entry_price = row[\"AbsolutePrice\"]\n            entry_index = index\n            holding = True\n        elif row[\"Predicted\"] == 0 and holding:\n            exit_price = row[\"AbsolutePrice\"]\n            exit_index = index\n            trade_return = (exit_price - entry_price) / entry_price\n            capital_before = capital\n            capital *= (1 + trade_return)\n            relative_profit = (capital - capital_before) / capital_before\n            holding = False\n            trades.append({\n                \"entry_index\": entry_index,\n                \"exit_index\": exit_index,\n                \"entry_price\": round(entry_price, 6),\n                \"exit_price\": round(exit_price, 6),\n                \"trade_return\": round(trade_return, 6),\n                \"capital_after_trade\": round(capital, 2),\n                \"relative_profit\": round(relative_profit, 6)\n            })\n            returns.append(trade_return)\n\n    if holding:\n        exit_price = df.iloc[-1][\"AbsolutePrice\"]\n        exit_index = df.index[-1]\n        trade_return = (exit_price - entry_price) / entry_price\n        capital_before = capital\n        capital *= (1 + trade_return)\n        relative_profit = (capital - capital_before) / capital_before\n        trades.append({\n            \"entry_index\": entry_index,\n            \"exit_index\": exit_index,\n            \"entry_price\": round(entry_price, 6),\n            \"exit_price\": round(exit_price, 6),\n            \"trade_return\": round(trade_return, 6),\n            \"capital_after_trade\": round(capital, 2),\n            \"relative_profit\": round(relative_profit, 6)\n        })\n        returns.append(trade_return)\n\n    trades_df = pd.DataFrame(trades)\n    return capital, (capital - initial_investment) / initial_investment, trades_df\n\n\ndef calculate_monthly_profits(df: pd.DataFrame, intervals_per_month: int) -> pd.DataFrame:\n    monthly_profits = []\n    hodl_profits = []\n\n    total_intervals = len(df)\n    num_months = total_intervals // intervals_per_month\n\n    for month in range(num_months):\n        start_idx = month * intervals_per_month\n        end_idx = start_idx + intervals_per_month - 1\n\n        month_data = df.iloc[start_idx:end_idx + 1]\n        _, monthly_profit, _ = evaluate_strategy(month_data)\n\n        start_price = month_data.iloc[0][\"AbsolutePrice\"]\n        end_price = month_data.iloc[-1][\"AbsolutePrice\"]\n        hodl_profit = (end_price / start_price - 1) * 100\n\n        monthly_profits.append(monthly_profit * 100)  # Convert to percentage\n        hodl_profits.append(hodl_profit)\n\n    # Calculate overall HODL profit for the entire dataset\n    overall_hodl_profit = (df.iloc[-1][\"AbsolutePrice\"] / df.iloc[0][\"AbsolutePrice\"] - 1) * 100\n\n    results_df = pd.DataFrame({\n        \"Month\": range(1, num_months + 1),\n        \"Strategy Profit (%)\": monthly_profits,\n",
        "middle": "\"HODL Profit (%)\": hodl_profits",
        "suffix": "    })\n\n    print(\"Monthly Strategy and HODL Profits:\")\n    print(results_df)\n\n    print(f\"Overall HODL Profit for the entire dataset: {overall_hodl_profit:.2f}%\")\n\n    return results_df\n\n\ndef main():\n    file_path = \"../results/outputs/test_pca_True_results_with_prices_max.csv\"\n    df = read_data(file_path)\n\n    if df.empty:\n        return\n\n    intervals_per_month = len(df)// 17\n    calculate_monthly_profits(df, intervals_per_month)\n\n    initial_investment = 1000\n    final_capital, total_return, trades_df = evaluate_strategy(df, initial_investment)\n\n    print(f\"Initial Investment: ${initial_investment:.2f}\")\n    print(f\"Final Capital: ${final_capital:.2f}\")\n    print(f\"Total Return: {total_return:.2%}\")\n    print(f\"Number of Trades: {len(trades_df)}\")\n\n    trades_df.to_csv(\"../results/outputs/trades_log.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "model_completion": "\"HODL Profit (%)\": hodl_profits,\n        \"Over",
        "Label": "correct"
    },
    {
        "file_name": "calculate_profit.py",
        "prefix": "import pandas as pd\nimport numpy as np\nfrom typing import Tuple, List\n\n\ndef read_data(file_path: str) -> pd.DataFrame:\n    try:\n        data = pd.read_csv(file_path)\n        return pd.DataFrame(data)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return pd.DataFrame()\n    except pd.errors.EmptyDataError:\n        print(f\"Error: The file at {file_path} is empty\")\n        return pd.DataFrame()\n    except pd.errors.ParserError:\n        print(f\"Error: Unable to parse the file at {file_path}\")\n        return pd.DataFrame()\n\n\ndef calculate_metrics(returns: List[float]) -> Tuple[float, float, float]:\n    returns = pd.Series(returns)\n    sharpe_ratio = np.sqrt(252) * returns.mean() / returns.std()\n    max_drawdown = (returns.cumsum() - returns.cumsum().cummax()).min()\n    total_return = (1 + returns).prod() - 1\n    return sharpe_ratio, max_drawdown, total_return\n\n\ndef evaluate_strategy(df: pd.DataFrame, initial_investment: float = 1000) -> Tuple[float, float, pd.DataFrame]:\n    holding = False\n    entry_price = 0\n    capital = initial_investment\n    trades = []\n    returns = []\n\n    for index, row in df.iterrows():\n        if row[\"Predicted\"] == 2 and not holding:\n            entry_price = row[\"AbsolutePrice\"]\n            entry_index = index\n            holding = True\n        elif row[\"Predicted\"] == 0 and holding:\n            exit_price = row[\"AbsolutePrice\"]\n            exit_index = index\n            trade_return = (exit_price - entry_price) / entry_price\n            capital_before = capital\n            capital *= (1 + trade_return)\n            relative_profit = (capital - capital_before) / capital_before\n            holding = False\n            trades.append({\n                \"entry_index\": entry_index,\n                \"exit_index\": exit_index,\n                \"entry_price\": round(entry_price, 6),\n                \"exit_price\": round(exit_price, 6),\n                \"trade_return\": round(trade_return, 6),\n                \"capital_after_trade\": round(capital, 2),\n                \"relative_profit\": round(relative_profit, 6)\n            })\n            returns.append(trade_return)\n\n    if holding:\n        exit_price = df.iloc[-1][\"AbsolutePrice\"]\n        exit_index = df.index[-1]\n        trade_return = (exit_price - entry_price) / entry_price\n        capital_before = capital\n        capital *= (1 + trade_return)\n        relative_profit = (capital - capital_before) / capital_before\n        trades.append({\n            \"entry_index\": entry_index,\n            \"exit_index\": exit_index,\n            \"entry_price\": round(entry_price, 6),\n            \"exit_price\": round(exit_price, 6),\n            \"trade_return\": round(trade_return, 6),\n            \"capital_after_trade\": round(capital, 2),\n            \"relative_profit\": round(relative_profit, 6)\n        })\n        returns.append(trade_return)\n\n    trades_df = pd.DataFrame(trades)\n    return capital, (capital - initial_investment) / initial_investment, trades_df\n\n\ndef calculate_monthly_profits(df: pd.DataFrame, intervals_per_month: int) -> pd.DataFrame:\n    monthly_profits = []\n    hodl_profits = []\n\n    total_intervals = len(df)\n    num_months = total_intervals // intervals_per_month\n\n    for month in range(num_months):\n        start_idx = month * intervals_per_month\n        end_idx = start_idx + intervals_per_month - 1\n\n        month_data = df.iloc[start_idx:end_idx + 1]\n        _, monthly_profit, _ = evaluate_strategy(month_data)\n\n        start_price = month_data.iloc[0][\"AbsolutePrice\"]\n        end_price = month_data.iloc[-1][\"AbsolutePrice\"]\n        hodl_profit = (end_price / start_price - 1) * 100\n\n        monthly_profits.append(monthly_profit * 100)  # Convert to percentage\n        hodl_profits.append(hodl_profit)\n\n    # Calculate overall HODL profit for the entire dataset\n    overall_hodl_profit = (df.iloc[-1][\"AbsolutePrice\"] / df.iloc[0][\"AbsolutePrice\"] - 1) * 100\n\n    results_df = pd.DataFrame({\n        \"Month\": range(1, num_months + 1),\n        \"Strategy Profit (%)\": monthly_profits,\n        \"HODL Profit (%)\": hodl_profits\n    })\n\n    print(\"Monthly Strategy and HODL Profits:\")\n    print(results_df)\n\n    print(f\"Overall HODL Profit for the entire dataset: {overall_hodl_profit:.2f}%\")\n\n    return results_df\n\n\ndef main():\n    file_path = \"../results/outputs/test_pca_True_results_with_prices_max.csv\"\n",
        "middle": "df = read_data(file_path)",
        "suffix": "\n    if df.empty:\n        return\n\n    intervals_per_month = len(df)// 17\n    calculate_monthly_profits(df, intervals_per_month)\n\n    initial_investment = 1000\n    final_capital, total_return, trades_df = evaluate_strategy(df, initial_investment)\n\n    print(f\"Initial Investment: ${initial_investment:.2f}\")\n    print(f\"Final Capital: ${final_capital:.2f}\")\n    print(f\"Total Return: {total_return:.2%}\")\n    print(f\"Number of Trades: {len(trades_df)}\")\n\n    trades_df.to_csv(\"../results/outputs/trades_log.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "model_completion": "df = read_data(file_path)\n    capital, relative_profit,",
        "Label": "correct"
    },
    {
        "file_name": "calculate_profit.py",
        "prefix": "import pandas as pd\nimport numpy as np\nfrom typing import Tuple, List\n\n\ndef read_data(file_path: str) -> pd.DataFrame:\n    try:\n        data = pd.read_csv(file_path)\n        return pd.DataFrame(data)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return pd.DataFrame()\n    except pd.errors.EmptyDataError:\n        print(f\"Error: The file at {file_path} is empty\")\n        return pd.DataFrame()\n    except pd.errors.ParserError:\n        print(f\"Error: Unable to parse the file at {file_path}\")\n        return pd.DataFrame()\n\n\ndef calculate_metrics(returns: List[float]) -> Tuple[float, float, float]:\n    returns = pd.Series(returns)\n    sharpe_ratio = np.sqrt(252) * returns.mean() / returns.std()\n    max_drawdown = (returns.cumsum() - returns.cumsum().cummax()).min()\n    total_return = (1 + returns).prod() - 1\n    return sharpe_ratio, max_drawdown, total_return\n\n\ndef evaluate_strategy(df: pd.DataFrame, initial_investment: float = 1000) -> Tuple[float, float, pd.DataFrame]:\n    holding = False\n    entry_price = 0\n    capital = initial_investment\n    trades = []\n    returns = []\n\n    for index, row in df.iterrows():\n        if row[\"Predicted\"] == 2 and not holding:\n            entry_price = row[\"AbsolutePrice\"]\n            entry_index = index\n            holding = True\n        elif row[\"Predicted\"] == 0 and holding:\n            exit_price = row[\"AbsolutePrice\"]\n            exit_index = index\n            trade_return = (exit_price - entry_price) / entry_price\n            capital_before = capital\n            capital *= (1 + trade_return)\n            relative_profit = (capital - capital_before) / capital_before\n            holding = False\n            trades.append({\n                \"entry_index\": entry_index,\n                \"exit_index\": exit_index,\n                \"entry_price\": round(entry_price, 6),\n                \"exit_price\": round(exit_price, 6),\n                \"trade_return\": round(trade_return, 6),\n                \"capital_after_trade\": round(capital, 2),\n                \"relative_profit\": round(relative_profit, 6)\n            })\n            returns.append(trade_return)\n\n    if holding:\n        exit_price = df.iloc[-1][\"AbsolutePrice\"]\n        exit_index = df.index[-1]\n        trade_return = (exit_price - entry_price) / entry_price\n        capital_before = capital\n        capital *= (1 + trade_return)\n        relative_profit = (capital - capital_before) / capital_before\n        trades.append({\n            \"entry_index\": entry_index,\n            \"exit_index\": exit_index,\n            \"entry_price\": round(entry_price, 6),\n            \"exit_price\": round(exit_price, 6),\n            \"trade_return\": round(trade_return, 6),\n            \"capital_after_trade\": round(capital, 2),\n            \"relative_profit\": round(relative_profit, 6)\n        })\n        returns.append(trade_return)\n\n    trades_df = pd.DataFrame(trades)\n    return capital, (capital - initial_investment) / initial_investment, trades_df\n\n\ndef calculate_monthly_profits(df: pd.DataFrame, intervals_per_month: int) -> pd.DataFrame:\n    monthly_profits = []\n    hodl_profits = []\n\n    total_intervals = len(df)\n    num_months = total_intervals // intervals_per_month\n\n    for month in range(num_months):\n        start_idx = month * intervals_per_month\n        end_idx = start_idx + intervals_per_month - 1\n\n        month_data = df.iloc[start_idx:end_idx + 1]\n        _, monthly_profit, _ = evaluate_strategy(month_data)\n\n        start_price = month_data.iloc[0][\"AbsolutePrice\"]\n        end_price = month_data.iloc[-1][\"AbsolutePrice\"]\n        hodl_profit = (end_price / start_price - 1) * 100\n\n        monthly_profits.append(monthly_profit * 100)  # Convert to percentage\n        hodl_profits.append(hodl_profit)\n\n    # Calculate overall HODL profit for the entire dataset\n    overall_hodl_profit = (df.iloc[-1][\"AbsolutePrice\"] / df.iloc[0][\"AbsolutePrice\"] - 1) * 100\n\n    results_df = pd.DataFrame({\n        \"Month\": range(1, num_months + 1),\n        \"Strategy Profit (%)\": monthly_profits,\n        \"HODL Profit (%)\": hodl_profits\n    })\n\n    print(\"Monthly Strategy and HODL Profits:\")\n    print(results_df)\n\n    print(f\"Overall HODL Profit for the entire dataset: {overall_hodl_profit:.2f}%\")\n\n    return results_df\n\n\ndef main():\n    file_path = \"../results/outputs/test_pca_True_results_with_prices_max.csv\"\n    df = read_data(file_path)\n\n    if df.empty:\n        return\n\n    intervals_per_month = len(df)// 17\n    calculate_monthly_profits(df, intervals_per_month)\n\n    initial_investment = 1000\n    final_capital, total_return, trades_df = evaluate_strategy(df, initial_investment)\n\n    print(f\"Initial Investment: ${initial_investment:.2f}\")\n    print(f\"Final Capital: ${final_capital:.2f}\")\n",
        "middle": "print(f\"Total Return: {total_return:.2%}\")",
        "suffix": "    print(f\"Number of Trades: {len(trades_df)}\")\n\n    trades_df.to_csv(\"../results/outputs/trades_log.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "model_completion": "print(f\"Total Return: {total_return:.2f}%\")",
        "Label": "correct"
    },
    {
        "file_name": "calculate_profit.py",
        "prefix": "import pandas as pd\n",
        "middle": "import numpy as np",
        "suffix": "from typing import Tuple, List\n\n\ndef read_data(file_path: str) -> pd.DataFrame:\n    try:\n        data = pd.read_csv(file_path)\n        return pd.DataFrame(data)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return pd.DataFrame()\n    except pd.errors.EmptyDataError:\n        print(f\"Error: The file at {file_path} is empty\")\n        return pd.DataFrame()\n    except pd.errors.ParserError:\n        print(f\"Error: Unable to parse the file at {file_path}\")\n        return pd.DataFrame()\n\n\ndef calculate_metrics(returns: List[float]) -> Tuple[float, float, float]:\n    returns = pd.Series(returns)\n    sharpe_ratio = np.sqrt(252) * returns.mean() / returns.std()\n    max_drawdown = (returns.cumsum() - returns.cumsum().cummax()).min()\n    total_return = (1 + returns).prod() - 1\n    return sharpe_ratio, max_drawdown, total_return\n\n\ndef evaluate_strategy(df: pd.DataFrame, initial_investment: float = 1000) -> Tuple[float, float, pd.DataFrame]:\n    holding = False\n    entry_price = 0\n    capital = initial_investment\n    trades = []\n    returns = []\n\n    for index, row in df.iterrows():\n        if row[\"Predicted\"] == 2 and not holding:\n            entry_price = row[\"AbsolutePrice\"]\n            entry_index = index\n            holding = True\n        elif row[\"Predicted\"] == 0 and holding:\n            exit_price = row[\"AbsolutePrice\"]\n            exit_index = index\n            trade_return = (exit_price - entry_price) / entry_price\n            capital_before = capital\n            capital *= (1 + trade_return)\n            relative_profit = (capital - capital_before) / capital_before\n            holding = False\n            trades.append({\n                \"entry_index\": entry_index,\n                \"exit_index\": exit_index,\n                \"entry_price\": round(entry_price, 6),\n                \"exit_price\": round(exit_price, 6),\n                \"trade_return\": round(trade_return, 6),\n                \"capital_after_trade\": round(capital, 2),\n                \"relative_profit\": round(relative_profit, 6)\n            })\n            returns.append(trade_return)\n\n    if holding:\n        exit_price = df.iloc[-1][\"AbsolutePrice\"]\n        exit_index = df.index[-1]\n        trade_return = (exit_price - entry_price) / entry_price\n        capital_before = capital\n        capital *= (1 + trade_return)\n        relative_profit = (capital - capital_before) / capital_before\n        trades.append({\n            \"entry_index\": entry_index,\n            \"exit_index\": exit_index,\n            \"entry_price\": round(entry_price, 6),\n            \"exit_price\": round(exit_price, 6),\n            \"trade_return\": round(trade_return, 6),\n            \"capital_after_trade\": round(capital, 2),\n            \"relative_profit\": round(relative_profit, 6)\n        })\n        returns.append(trade_return)\n\n    trades_df = pd.DataFrame(trades)\n    return capital, (capital - initial_investment) / initial_investment, trades_df\n\n\ndef calculate_monthly_profits(df: pd.DataFrame, intervals_per_month: int) -> pd.DataFrame:\n    monthly_profits = []\n    hodl_profits = []\n\n    total_intervals = len(df)\n    num_months = total_intervals // intervals_per_month\n\n    for month in range(num_months):\n        start_idx = month * intervals_per_month\n        end_idx = start_idx + intervals_per_month - 1\n\n        month_data = df.iloc[start_idx:end_idx + 1]\n        _, monthly_profit, _ = evaluate_strategy(month_data)\n\n        start_price = month_data.iloc[0][\"AbsolutePrice\"]\n        end_price = month_data.iloc[-1][\"AbsolutePrice\"]\n        hodl_profit = (end_price / start_price - 1) * 100\n\n        monthly_profits.append(monthly_profit * 100)  # Convert to percentage\n        hodl_profits.append(hodl_profit)\n\n    # Calculate overall HODL profit for the entire dataset\n    overall_hodl_profit = (df.iloc[-1][\"AbsolutePrice\"] / df.iloc[0][\"AbsolutePrice\"] - 1) * 100\n\n    results_df = pd.DataFrame({\n        \"Month\": range(1, num_months + 1),\n        \"Strategy Profit (%)\": monthly_profits,\n        \"HODL Profit (%)\": hodl_profits\n    })\n\n    print(\"Monthly Strategy and HODL Profits:\")\n    print(results_df)\n\n    print(f\"Overall HODL Profit for the entire dataset: {overall_hodl_profit:.2f}%\")\n\n    return results_df\n\n\ndef main():\n    file_path = \"../results/outputs/test_pca_True_results_with_prices_max.csv\"\n    df = read_data(file_path)\n\n    if df.empty:\n        return\n\n    intervals_per_month = len(df)// 17\n    calculate_monthly_profits(df, intervals_per_month)\n\n    initial_investment = 1000\n    final_capital, total_return, trades_df = evaluate_strategy(df, initial_investment)\n\n    print(f\"Initial Investment: ${initial_investment:.2f}\")\n    print(f\"Final Capital: ${final_capital:.2f}\")\n    print(f\"Total Return: {total_return:.2%}\")\n    print(f\"Number of Trades: {len(trades_df)}\")\n\n    trades_df.to_csv(\"../results/outputs/trades_log.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "model_completion": "# Create a Pandas dataframe from some data.\ndf = pd.DataFrame({'",
        "Label": "incorrect"
    },
    {
        "file_name": "calculate_profit.py",
        "prefix": "import pandas as pd\nimport numpy as np\nfrom typing import Tuple, List\n\n\ndef read_data(file_path: str) -> pd.DataFrame:\n    try:\n        data = pd.read_csv(file_path)\n        return pd.DataFrame(data)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return pd.DataFrame()\n    except pd.errors.EmptyDataError:\n        print(f\"Error: The file at {file_path} is empty\")\n        return pd.DataFrame()\n    except pd.errors.ParserError:\n        print(f\"Error: Unable to parse the file at {file_path}\")\n        return pd.DataFrame()\n\n\ndef calculate_metrics(returns: List[float]) -> Tuple[float, float, float]:\n    returns = pd.Series(returns)\n    sharpe_ratio = np.sqrt(252) * returns.mean() / returns.std()\n    max_drawdown = (returns.cumsum() - returns.cumsum().cummax()).min()\n    total_return = (1 + returns).prod() - 1\n    return sharpe_ratio, max_drawdown, total_return\n\n\ndef evaluate_strategy(df: pd.DataFrame, initial_investment: float = 1000) -> Tuple[float, float, pd.DataFrame]:\n    holding = False\n    entry_price = 0\n    capital = initial_investment\n    trades = []\n    returns = []\n\n    for index, row in df.iterrows():\n        if row[\"Predicted\"] == 2 and not holding:\n            entry_price = row[\"AbsolutePrice\"]\n            entry_index = index\n            holding = True\n        elif row[\"Predicted\"] == 0 and holding:\n            exit_price = row[\"AbsolutePrice\"]\n            exit_index = index\n            trade_return = (exit_price - entry_price) / entry_price\n            capital_before = capital\n            capital *= (1 + trade_return)\n            relative_profit = (capital - capital_before) / capital_before\n            holding = False\n            trades.append({\n                \"entry_index\": entry_index,\n                \"exit_index\": exit_index,\n                \"entry_price\": round(entry_price, 6),\n                \"exit_price\": round(exit_price, 6),\n                \"trade_return\": round(trade_return, 6),\n                \"capital_after_trade\": round(capital, 2),\n                \"relative_profit\": round(relative_profit, 6)\n            })\n            returns.append(trade_return)\n\n    if holding:\n        exit_price = df.iloc[-1][\"AbsolutePrice\"]\n        exit_index = df.index[-1]\n        trade_return = (exit_price - entry_price) / entry_price\n        capital_before = capital\n        capital *= (1 + trade_return)\n        relative_profit = (capital - capital_before) / capital_before\n        trades.append({\n            \"entry_index\": entry_index,\n            \"exit_index\": exit_index,\n            \"entry_price\": round(entry_price, 6),\n            \"exit_price\": round(exit_price, 6),\n            \"trade_return\": round(trade_return, 6),\n            \"capital_after_trade\": round(capital, 2),\n            \"relative_profit\": round(relative_profit, 6)\n        })\n        returns.append(trade_return)\n\n    trades_df = pd.DataFrame(trades)\n    return capital, (capital - initial_investment) / initial_investment, trades_df\n\n\ndef calculate_monthly_profits(df: pd.DataFrame, intervals_per_month: int) -> pd.DataFrame:\n    monthly_profits = []\n    hodl_profits = []\n\n    total_intervals = len(df)\n    num_months = total_intervals // intervals_per_month\n\n    for month in range(num_months):\n",
        "middle": "start_idx = month * intervals_per_month",
        "suffix": "        end_idx = start_idx + intervals_per_month - 1\n\n        month_data = df.iloc[start_idx:end_idx + 1]\n        _, monthly_profit, _ = evaluate_strategy(month_data)\n\n        start_price = month_data.iloc[0][\"AbsolutePrice\"]\n        end_price = month_data.iloc[-1][\"AbsolutePrice\"]\n        hodl_profit = (end_price / start_price - 1) * 100\n\n        monthly_profits.append(monthly_profit * 100)  # Convert to percentage\n        hodl_profits.append(hodl_profit)\n\n    # Calculate overall HODL profit for the entire dataset\n    overall_hodl_profit = (df.iloc[-1][\"AbsolutePrice\"] / df.iloc[0][\"AbsolutePrice\"] - 1) * 100\n\n    results_df = pd.DataFrame({\n        \"Month\": range(1, num_months + 1),\n        \"Strategy Profit (%)\": monthly_profits,\n        \"HODL Profit (%)\": hodl_profits\n    })\n\n    print(\"Monthly Strategy and HODL Profits:\")\n    print(results_df)\n\n    print(f\"Overall HODL Profit for the entire dataset: {overall_hodl_profit:.2f}%\")\n\n    return results_df\n\n\ndef main():\n    file_path = \"../results/outputs/test_pca_True_results_with_prices_max.csv\"\n    df = read_data(file_path)\n\n    if df.empty:\n        return\n\n    intervals_per_month = len(df)// 17\n    calculate_monthly_profits(df, intervals_per_month)\n\n    initial_investment = 1000\n    final_capital, total_return, trades_df = evaluate_strategy(df, initial_investment)\n\n    print(f\"Initial Investment: ${initial_investment:.2f}\")\n    print(f\"Final Capital: ${final_capital:.2f}\")\n    print(f\"Total Return: {total_return:.2%}\")\n    print(f\"Number of Trades: {len(trades_df)}\")\n\n    trades_df.to_csv(\"../results/outputs/trades_log.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "model_completion": "month_start = month * intervals_per_month\n        month_end = (month",
        "Label": "partial"
    },
    {
        "file_name": "display_results.py",
        "prefix": "from src.utils.data_saving_and_displaying import display_npy_file\n\n\n",
        "middle": "display_npy_file('../results/outputs/test_actuals.npy')",
        "suffix": "display_npy_file('../results/outputs/test_predictions.npy')",
        "model_completion": "def test_display_npy_file():\n    \"\"\"\n    Tests the display_",
        "Label": "incorrect"
    },
    {
        "file_name": "display_results.py",
        "prefix": "from src.utils.data_saving_and_displaying import display_npy_file\n\n\n",
        "middle": "display_npy_file('../results/outputs/test_actuals.npy')",
        "suffix": "display_npy_file('../results/outputs/test_predictions.npy')",
        "model_completion": "def display_npy_file_with_labels(npy_file, labels):",
        "Label": "incorrect"
    }
]